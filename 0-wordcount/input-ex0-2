Human Detection Using Partial Least Squares Analysis
William Robson Schwartz, Aniruddha Kembhavi, David Harwood, Larry S. Davis University of Maryland, A.V.Williams Building, College Park, MD 20742
schwartz@cs.umd.edu, anikem@umd.edu, harwood@umiacs.umd.edu, lsd@cs.umd.edu

Abstract
Signiﬁcant research has been devoted to detecting people in images and videos. In this paper we describe a human detection method that augments widely used edge-based features with texture and color information, providing us with a much richer descriptor set. This augmentation results in an extremely high-dimensional feature space (more than 170,000 dimensions). In such high-dimensional spaces, classical machine learning algorithms such as SVMs are nearly intractable with respect to training. Furthermore, the number of training samples is much smaller than the dimensionality of the feature space, by at least an order of magnitude. Finally, the extraction of features from a densely sampled grid structure leads to a high degree of multicollinearity. To circumvent these data characteristics, we employ Partial Least Squares (PLS) analysis, an efﬁcient dimensionality reduction technique, one which preserves signiﬁcant discriminative information, to project the data onto a much lower dimensional subspace (20 dimensions, reduced from the original 170,000). Our human detection system, employing PLS analysis over the enriched descriptor set, is shown to outperform state-of-the-art techniques on three varied datasets including the popular INRIA pedestrian dataset, the low-resolution gray-scale DaimlerChrysler pedestrian dataset, and the ETHZ pedestrian dataset consisting of full-length videos of crowded scenes.

Figure 1. Image demonstrating the performance of our system in a complex scene. The image (689 × 480 pixels) is scanned at 10 scales to search for humans of multiple sizes. We achieve minimal false alarms even though the number of detection windows is 44, 996 (best visualized in color).

1. Introduction
Effective techniques for human detection are of special interest in computer vision since many applications involve people’s locations and movements. Thus, signiﬁcant research has been devoted to detecting, locating and tracking people in images and videos. Over the last few years the problem of detecting humans in single images has received considerable interest. Variations in illumination, shadows, and pose, as well as frequent inter- and intra-person occlusion render this a challenging task. Figure 1 shows an image of a particularly challenging scene with a large number of persons, overlaid with the results of our system. Two main approaches to human detection have been explored over the last few years. The ﬁrst class of meth-

ods consists of a generative process where detected parts of the human body are combined according to a prior human model. The second class of methods considers purely statistical analysis that combine a set of low-level features within a detection window to classify the window as containing a human or not. The method presented in this paper belongs to the latter category. Dalal and Triggs [5] proposed using grids of Histograms of Oriented Gradient (HOG) descriptors for human detection, and obtained good results on multiple datasets. The HOG feature looks at the spatial distribution of edge orientations. However, this may ignore some other useful sources of information, thus leading to a number of false positive detections such as the ones shown in Figure 2. Our analysis shows that information such as the homogeneity of human clothing, color, particularly skin color, typical textures of human clothing, and background textures complement the HOG features very well. When combined, this richer set of descriptors helps improve the detection results signiﬁcantly. A consequence of such feature augmentation is an extremely high dimensional feature space (more than 170, 000 dimensions), rendering many classical machine learning techniques such as Support Vector Machines (SVM) intractable. In contrast, the number of samples in our training dataset is much smaller (almost 20 times smaller than

Figure 2. False positives obtained when only edge information (using HOG features) is considered.

the dimensionality). Furthermore, our features are extracted from neighboring blocks within a detection window, which increases the multicollinearity of the feature set. The nature of our proposed feature set makes an ideal setting for a statistical technique known as Partial Least Squares (PLS) regression [23]. PLS is a class of methods for modeling relations between sets of observations by means of latent variables. Although originally proposed as a regression technique, PLS can be also be used as a class aware dimensionality reduction tool. We use PLS to project our high dimensional feature vectors onto a subspace of dimensionality as low as 20. In such low dimensional spaces, standard machine learning techniques such as quadratic classiﬁers and SVMs can be used for our classiﬁcation task. Our proposed human detection approach outperforms state-of-the-art approaches on multiple standard datasets. Since the number of detection windows within an image is very high (tens of thousands for a 640 × 480 image scanned at multiple scales), it is crucial to obtain good detection results at very small false alarm rates. On the popular INRIA person dataset [5], we obtain superior results at false alarm rates as low as 10−5 and 10−6 false positives per window (FPPW). We also test on the ETHZ pedestrian dataset [7] consisting of full-length videos captured in crowded scenes. Even though we do not retrain our human detector using the provided training set (but use the detector trained on the INRIA training set), our method outperforms other approaches that utilize many more sources of information such as depth maps, ground-plane estimation, and occlusion reasoning [7]. Finally, we also demonstrate our method on detecting humans at very low resolutions (18 × 36 pixels) using the DaimlerChrysler dataset [18].

Using low-level features such as intensity, gradient, and spatial location combined by a covariance matrix, Tuzel et al. [22] improve the results obtained by Dalal and Triggs. Since the covariance matrices do not lie in a vector space, the classiﬁcation is performed using LogitBoost classiﬁers combined with a rejection cascade designed to accommodate points lying on a Riemannian manifold. Mu et al. [17] propose a variation of local binary patterns to overcome some drawbacks of HOG, such as lack of color information. Chen and Chen [4] combine intensity-based rectangle features and gradient-based features using a cascaded structure for detecting humans. Applying combination of edgelets [25], HOG descriptors [5], and covariance descriptors [22], Wu and Nevatia [26] describe a cascade-based approach where each weak classiﬁer corresponds to a subregion within the detection window from which different types of features are extracted. Dollar et al. [6] propose a method to learn classiﬁers for individual components and combine them into an overall classiﬁer. The work of Maji et al. [14] uses features based on a multi-level version of HOG and histogram intersection kernel SVM based on the spatial pyramid match kernel [12]. Employing part-based detectors, Mikolajczyk et al. [15] divide the human body into several parts and apply a cascade of detectors for each part. Shet and Davis [20] apply logical reasoning to exploit contextual information, augmenting the output of low-level detectors. Based on deformable parts, Felzenszwalb et al. [9] simultaneously learn part and object models and apply them to person detection, among other applications. Tran and Forsyth [21] use an approach that mixes a part-based method and a subwindowbased method into a two stage method. Their approach ﬁrst estimates a possible conﬁguration of the person inside the detection window, and then extracts features for each part resulting from the estimation. Similarly, Lin and Davis [13] propose a pose-invariant feature extraction method for simultaneous human detection and segmentation, where descriptors are computed adaptively based on human poses.

3. Proposed Method
Previous studies [14, 22, 26] have shown that signiﬁcant improvement in human detection can be achieved using different types (or combinations) of low-level features. A strong set of features provides high discriminatory power, reducing the need for complex classiﬁcation methods. Humans in standing positions have distinguishing characteristics. First, strong vertical edges are present along the boundaries of the body. Second, clothing is generally uniform. Clothing textures are different from natural textures observed outside of the body due to constraints on the manufacturing of printed cloth. Third, the ground is composed mostly of uniform textures. Finally, discriminatory color information is found in the face/head regions. Thus, edges, colors and textures capture important cues for discriminating humans from the background. To capture these cues, the low-level features we employ are the original HOG descriptors with additional color information,

2. Related Work
The work of Dalal and Triggs [5] is notable because it was the ﬁrst paper to report impressive results on human detection. Their work uses HOG as low-level features, which were shown to outperform features such as wavelets [16], PCA-SIFT [11] and shape contexts [2]. To improve detection speed, Zhu et al. [28] propose a rejection cascade using HOG features. Their method considers blocks of different sizes, and to train the classiﬁer for each stage, a small subset of blocks is selected randomly. Also based on HOG features, Zhang et al. [27] propose a multi-resolution framework to reduce the computational cost. Begard et al. [1] address the problem of real-time pedestrian detection by considering different implementations of the AdaBoost algorithm.

called color frequency, and texture features computed from co-occurrence matrices. To handle the high dimensionality resulting from the combination of features, PLS is employed as a dimensionality reduction technique. PLS is a powerful technique that provides dimensionality reduction for even hundreds of thousands of variables, accounting for class labels in the process. The latter point is in contrast to traditional dimensionality reduction techniques such as Principal Component Analysis (PCA). The steps performed in our detection method are the following. For each detection window in the image, features extracted using original HOG, color frequency, and co-occurrence matrices are concatenated and analyzed by the PLS model to reduce dimensionality, resulting in a low dimensional vector. Then, a simple and efﬁcient classiﬁer is used to classify this vector as either a human or non-human. These steps are explained in the following subsections.

color frequency increases detection performance. Once the feature extraction process is performed for all blocks inside a detection window di , features are concatenated creating an extremely high-dimensional feature vector vi . Then, vi is projected onto a set of weight vectors (discussed in the next section), which results in a low dimensional representation that can be handled by classiﬁcation methods.

3.2. Partial Least Squares for Dimension Reduction
Partial least squares is a method for modeling relations between sets of observed variables by means of latent variables. The basic idea of PLS is to construct new predictor variables, latent variables, as linear combinations of the original variables summarized in a matrix X of descriptor variables (features) and a vector y of response variables (class labels). While additional details regarding PLS methods can be found in [19], a brief mathematical description of the procedure is provided below. Let X ⊂ Rm denote an m-dimensional space of feature vectors and similarly let Y ⊂ R be a 1-dimensional space representing the class labels. Let the number of samples be n. PLS decomposes the zero-mean matrix X (n × m) and zero-mean vector y (n × 1) into X = TPT + E y = U qT + f where T and U are n × p matrices containing p extracted latent vectors, the (m × p) matrix P and the (1 × p) vector q represent the loadings and the n × m matrix E and the n × 1 vector f are the residuals. The PLS method, using the nonlinear iterative partial least squares (NIPALS) algorithm [23], constructs a set of weight vectors (or projection vectors) W = {w1 , w2 , . . . wp } such that [cov(ti , ui )]2 = max [cov(Xwi , y)]2
|wi |=1

3.1. Feature Extraction
We decompose a detection window, di , into overlapping blocks and extract a set of features for each block to construct the feature vector vi . To capture texture, we extract features from cooccurrence matrices [10], a method widely used for texture analysis. Co-occurrence matrices represent second order texture information - i.e., the joint probability distribution of gray-level pairs of neighboring pixels in a block. We use 12 descriptors: angular second-moment, contrast, correlation, variance, inverse difference moment, sum average, sum variance, sum entropy, entropy, difference variance, difference entropy, and directionality [10]. Co-occurrence features are useful in human detection since they provide information regarding homogeneity and directionality of patches. In general, a person wears clothing composed of homogeneous textured regions and there is a signiﬁcant difference between the regularity of clothing texture and background textures. Edge information is captured using histograms of oriented gradients. HOG captures edge or gradient structures that are characteristic of local shape [5]. Since the histograms are computed for regions of a given size within the detection window, HOG is robust to some location variability of body parts. HOG is also invariant to rotations smaller than the orientation bin size. The last type of information captured is color. Although colors may not be consistent due to variability in clothing, certain dominant colors are more often observed in humans, mainly in the face/head regions. In order to incorporate color we used the original HOG to extract a descriptor called color frequency. In HOG, the orientation of the gradient for a pixel is chosen from the color band corresponding to the highest gradient magnitude. Some color information is captured by the number of times each color band is chosen. Therefore, we construct a three bin histogram that tabulates the number of times each color band is chosen. In spite of its simplicity, experimental results have shown that

where ti is the i-th column of matrix T , ui the i-th column of matrix U and cov(ti , ui ) is the sample covariance between latent vectors ti and ui . After the extraction of the latent vectors ti and ui , the matrix X and vector y are deﬂated by subtracting their rank-one approximations based on ti and ui . This process is repeated until the desired number of latent vectors had been extracted. The dimensionality reduction is performed by projecting the feature vector vi , extracted from a detection window di , onto the weight vectors W = {w1 , w2 , . . . wp }, obtaining the latent vector zi (1 × p) as a result. This vector is used in classiﬁcation. The difference between PLS and PCA is that the former creates orthogonal weight vectors by maximizing the covariance between elements in X and y. Thus, PLS not only considers the variance of the samples but also considers the class labels. Fisher Discriminant Analysis (FDA) is, in this way, similar to PLS. However, FDA has the limitation that

after dimensionality reduction, there are only c − 1 meaningful latent variables, where c is the number of classes being considered. Additionally, when the number of features exceeds the number of samples, the covariance estimates do not have full rank and the weight vectors cannot be extracted.

4. Experiments
We now present experiments to evaluate several aspects of our proposed approach. First, we demonstrate the need for dimensionality reduction and the advantages of using PLS for this purpose. Second, we evaluate the features used in our system. Third, we compare various classiﬁers that can be used to classify the data in the low dimensional subspace. Fourth, we discuss the computational cost of our method. Finally, we compare the proposed system to state-of-the-art algorithms on several datasets considering cropped as well as full images. Experimental Setup. For co-occurrence feature extraction we use block sizes of 16 × 16 and 32 × 32 with shifts of 8 and 16 pixels, respectively. We work in the HSV color space. For each color band, we create four co-occurrence matrices, one for each of the (0◦ , 45◦ , 90◦ , and 135◦ ) directions. The displacement considered is 1 pixel and each color band is quantized into 16 bins. 12 descriptors mentioned earlier are then extracted from each co-occurrence matrix. This results in 63, 648 features. We calculate HOG features similarly to Zhu et al. [28], where blocks with sizes ranging from 12 × 12 to 64 × 128 are considered. In our conﬁguration there are 2, 748 blocks. For each block, 36 features are extracted, resulting in a total of 98, 928 features. In addition, we use the same set of blocks to extract features using the color frequency method. This results in three features per block, and the total number of resulting features is 8, 244. Aggregating across all three feature channels, the feature vector describing each detection window contains 170, 820 elements. We estimate the parameters of our system using a 10-fold cross-validation procedure on the training dataset provided by INRIA Person Dataset [5]. The INRIA person dataset provides a training dataset containing 2416 positive samples of size 64 × 128 pixels and images containing no humans, used to obtain negative exemplars. We sample this set to obtain our validation set containing 2000 positive samples and 10000 negative samples. In sections 4.1 to 4.4 our experiments are performed using the INRIA person dataset. Experimental results using INRIA Person Dataset are presented using detection error tradeoff (DET) curves on log-log scales. The x-axis corresponds to false positives per window (FPPW), deﬁned by FalsePos/(TrueNeg + FalsePos) and the y-axis shows the miss rate, deﬁned by FalseNeg/(FalseNeg + TruePos). To clarify the results shown throughout the paper, curves where the lowest FPPW is 10−4 are obtained using the training data, while curves where the lowest FPPW is 10−6 are obtained using the testing data. All experiments were conducted on an Intel Xeon 5160, 3 GHz dual core processor with 8GB of RAM running Linux operating system.

3.3. Speed Issues
Although detection results can be improved by utilizing overlapping blocks for low-level feature extraction within the detection window, the dimensionality of the feature vector becomes extremely high. As a result, the speed of the human detector decreases signiﬁcantly due to the time needed to extract features and project them. To overcome this problem, we employ a two-stage approach. In a fast ﬁrst stage, based on a small number of features, the majority of detection windows (those with low probability of containing humans) are discarded. The remaining windows are evaluated during a second stage where the complete set of features allows challenging samples to be correctly classiﬁed. The reduced set of features used during the ﬁrst stage is obtained by selecting representative blocks within the detection window. We use a PLS-based feature selection method called variable importance on projection (VIP) [24] to do this. VIP provides a score for each feature, so that it is possible to rank the features according to their predictive power in the PLS model (the higher the score the more importance a feature presents). VIP for the j-th feature is deﬁned as
p p 2 2 bk wjk / k=1 k=1

VIPj =

m

b2 k

where m denotes the number of features, wjk is the j-th element of vector wk , and bk is the regression weight for the k-th latent variable, bk = uT tk . k The speed improvements are twofold: (i) reducing the overall number of feature computations; (ii) reducing the time to create the data structure for a block, i.e. computing a co-occurrence matrix from which features are extracted. If features were selected individually, then a data structure might need to be constructed for a block to compute only one feature. To avoid that, we select features based on blocks. This way, data structures for a block are only built if several features within the block present some importance. To obtain the relative discriminative power among blocks we build a PLS model for each block, from which only the ﬁrst latent variable is considered (since PLS considers class labels, the ﬁrst latent variable can be used as a clue about how well that block contributes to the detection). A global PLS model is built using as input only the ﬁrst latent variable of every block. Then, VIP scores are computed with respect to this PLS model, in this way, blocks can be ranked according to their importance in detection. Finally, the features used in the ﬁrst stage of our approach are those computed from blocks having high rank.

4.1. Dimensionality Reduction
PLS+QDA Vs SVM. We ﬁrst examine the feasibility of applying support vector machines (SVM) directly on

First two dimensions for PCA
0.03 0.03 0.02 second dimension 0.01 0 −0.01 −0.02 −0.03 −0.02

First two dimensions for PLS

0.02 second dimension

0.01

0

−0.01

non−human human −0.03 −0.02 −0.01 0 first dimension 0.01 0.02

non−human human −0.01 0 0.01 first dimension 0.02 0.03

−0.02 −0.04

(a) PCA - ﬁrst two dimensions
Detection Error Tradeoff
0.5 10 20 30 50 80 100 120 140 160 180 200

(b) PLS - ﬁrst two dimensions
Detection Error Tradeoff
0.5 2 4 10 15 20 25 30 35 40 60

0.2 0.1

0.2 0.1 miss rate 0.05

0.05

0.02 0.01

0.02 0.01

10

−4

10

−3

10

−2

10

−1

false positive per window (FPPW)

10

−4

10 10 false positive per window (FPPW)

−3

−2

10

−1

(c) PCA - cross-validation

(d) PLS - cross-validation

Figure 3. Comparison of PCA and PLS for dimensionality reduction. (a-b) projection of the ﬁrst two dimensions of the training samples for one of the models learned in the cross-validation. (cd) DET curves according to the number of dimensions used to train the classiﬁer (best visualized in color).

performance of the system drops when the number of latent variables is increased beyond 20. This can be attributed to overﬁtting of the data caused by using a larger number of latent variables. The results achieved while using the ﬁrst 20 latent variables are the best results obtained over both subspaces (0.8% miss rate at 10−4 FPPW). The best performance on the PCA subspace is obtained for a dimensionality of 180 (1.8% miss rate at 10−4 FPPW). As the dimensionality of the subspace increases, the time required to project the high dimensional feature vectors onto the low dimensional space also increases. On our computer, projecting the feature vector for a single window onto a 180 dimensional PCA subspace takes 0.0264 seconds while it takes 0.0032 seconds to project onto the 20 dimensional PLS subspace. Since an image contains several thousand windows, a computational cost of 0.0264 seconds/window is substantially worse than that for PLS. Thus, in addition to the superior performance, the computational cost of projection makes PLS more suitable for our application than PCA. Figure 3(a) and (b) show the training dataset projected onto the ﬁrst two dimensions for PLS and PCA. PLS clearly achieves better class separation than PCA.

the high dimensional feature space (170, 820 features per sample). Table 1 shows the comparison between time required to train a linear SVM and the time required to train a PLS model along with a Quadratic Discriminant Analysis (QDA) model (we use the QDA classiﬁer, but in later subsections we provide a comparison to other classiﬁers as well). We used the LIBSVM [3] package for this purpose. As the number of training samples is increased, the training time also increases. For more than 1800 samples we were unable to train a linear SVM since the procedure ran out of memory. In addition, the computational cost to learn a PLS model and train a QDA classiﬁer is an order of magnitude smaller than the cost for training an SVM. These results indicate that for such a high dimensional space, it is more suitable to project the data onto a low dimensional subspace and then learn a classiﬁer.
# samples 200 600 1000 1400 1800 2200 11370 PLS + QDA 23.63 62.62 97.38 135.81 174.57 213.93 813.03 SVM 131.72 733.63 1693.50 2947.51 4254.63 -

Miss Rate

4.2. Feature Evaluation
Comparing features. Figure 4(a) shows the results of the three classes of features used in our system as well as the combined performance. We show results combining the HOG and color frequency features to demonstrate the positive contribution of the color features. A signiﬁcant improvement is achieved when all features are combined. Analysis of the PLS Weight Vectors. In this experiment, we perform an analysis of the contribution of each feature channel based on the weights of the PLS weight vectors used to project the features onto the low dimensional subspace. We use the same idea as described in Section 3.3. For a given block in the detection window, we create a PLS model for each feature channel. Then, using only the ﬁrst latent variable for every block, we learn a global PLS model. Figure 5 shows the weights for the ﬁrst ﬁve projection vectors of this global PLS model. The features considered are HOG, co-occurrence extracted from color bands H, S and V, and the color frequency. Figure 5 shows how each feature channel (edge, texture, color) provides information from different regions within the detection window. This supports our claim that the considered features complement each other, leading to an improvement over single-feature-based methods. For example, the ﬁrst weight vector of the HOG feature set captures information about the body shape due to the presence of edges. Co-occurrence matrix features from color band H extract information around the body silhouette. Color bands S and V provide information about the head and homogeneous parts inside the body, respectively. Except for the ﬁrst weight vector, color frequency features are able to identify regions located in the head due to similarity of the dominant colors in that region (skin color).

Table 1. Time, in seconds, to train SVM and PLS + QDA models. The number of features per sample is 170,820. The training time increases with an increase in the number of training samples.

PLS Vs PCA. We now establish a baseline using Principal Component Analysis (PCA) to perform linear dimensionality reduction and compare its results to PLS. Figures 3(c) and (d) show the DET curves obtained for a QDA classiﬁer in the PCA subspace as well as in the PLS subspace. It is interesting to note that while the best results are obtained by using the ﬁrst 20 PLS latent variables, the

Detection Error Tradeoff
0.5 co−occurrence HOG + color frequency HOG all features combined

FPPW vs. Miss Rate
0.5 QDA LDA Logistic Regression Linear SVM Kernel SVM 0.5

Detection Error Tradeoff
full set of features for all detection windows two−stage approach 0.2

0.2

0.2 0.1

miss rate

miss rate

miss rate
−4 −3 −2 −1

0.1

0.1

0.05

0.05

0.05

0.02
0.02

0.01

0.02

0.01 −6 10

10

−5

10

−4

10

−3

10

−2

10

−1

10

10

10

10

0.01 −6 10

10

−5

10

−4

10

−3

10

−2

10

−1

false positives per window (FPPW)

false positives per window (FPPW)

false positives per window (FPPW)

(a)

(b)

(c)

Figure 4. (a) results obtained by using different features and combination of all three feature channels used by this work; (b) comparison of several classiﬁcation methods for the low dimensional PLS subspace; (c) results after adding two stages compared to results obtained without speed optimization.

tures computed in the ﬁrst stage, we rank blocks according to their VIP scores and then select only those features in blocks with higher rankings. Using 10-fold cross-validation in the training set, we select a subset of blocks containing 3, 573 features per detection window, together with a probability threshold to decide whether a detection window needs to be considered for the second stage. It is important to note that the use of the ﬁrst stage alone achieves poor results for low false alarm rates. Therefore, for the detection windows not discarded in the ﬁrst stage (approximately 3% for the INRIA person dataset), the complete feature set is computed. For the testing set of the INRIA person dataset, the results shown in Figure 4(c) indicate no degradation in performance at low false alarm rates when the two-stage approach is used, as compared to computing the full set of features for all detection windows. After speeding the process up using our two-stage method, we were able to process 2929 detection windows per second.

4.5. Evaluation and Comparisons
In this section we evaluate the proposed system on different datasets and compare it to state-of-the-art methods. INRIA Person Dataset. The INRIA person dataset [5] provides both training and testing sets containing positive samples of size 64 × 128 pixels and negatives images (containing no humans). To estimate weight vectors (PLS model) and train the quadratic classiﬁer we employ the following procedure. First, all 2416 positive training samples and 5000 of the negative detection windows, sampled randomly from training images, are used. Once the ﬁrst model is created, we use it to classify negative windows in the training set. The misclassiﬁed windows are added into the 5000 negative windows and a new PLS model and new classiﬁer parameters are estimated. This process is repeated a few times and takes approximately one hour. Our ﬁnal PLS model considers 8954 negative and 2416 positive samples, using 20 weight vectors (as discussed in section 4.1). Figure 6(a) compares results obtained by the proposed approach to methods published previously. Our results were obtained using 1126 positive testing samples and by shifting the detection windows by 8 pixels in the negative testing images, all of which are available in the dataset. While we were able to run the implementations for methods [5, 22], curves for methods [6, 13, 14, 26] were obtained from their

Figure 5. Weight vectors for different features within the detection window. Red indicates high importance, blue low (the plots are in the same scale and normalized to interval [0, 1]).

4.3. Classiﬁcation in Low Dimensional Space
To evaluate the classiﬁcation in the low dimensional subspace, we compare the performance of several classiﬁers using the 10-fold cross-validation described earlier. Figure 4(b) shows the results. According to the results, QDA classiﬁer, kernel SVM and linear SVM achieved comparable performance in low dimensional subspace. Due to its simplicity, we have chosen to use QDA in our system. PLS tends to produce weight vectors that provide a good separation of the two classes for the human detection problem, as shown in Figure 3(b). This enables us to use simple classiﬁers in the low dimensional subspace.

4.4. Computational Cost
We accelerate the process using the two-stage approach described in Section 3.3. To reduce the number of fea-

INRIA Pedestrian Dataset
Detection Error Tradeoff
0.5 1

DaimlerChrysler Dataset
Receiver Operating Characteristic
1 0.9 0.8 0.8 0.7 0.6 0.6

ETHZ Pedestrian Dataset
Seq. #1 (999 frames, 5193 annotations)
1

Seq. #2 (450 frames, 2359 annotations)
1 0.9 0.8 0.7 0.6

Seq. #3 (354 frames, 1828 annotations)
0.9 0.8 0.7 0.6

Performance Evaluation

0.2

0.1

detection rate

miss rate

our approach Lin & Davis [13] Tuzel et al. [22] Dalal & Triggs [5] Maji et al. [14] Wu, Nevatia [26] Dollar et al. [6]

Ess et al. [7] our method Ess et al. [8]

Ess et al. [7] our method

Ess et al. [7] our method

recall

recall

recall

0.5 0.4 0.3

0.5 0.4 0.3 0.2 0.1

0.5 0.4 0.3 0.2 0.1

0.05

0.4

0.02

0.2

our approach Maji et al. [14] Munder, Gavrila [18]
0 0.05 0.1 0.15 0.2

0.2 0.1 0 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5

0.01 −6 10

10

−5

10

−4

10

−3

10

−2

10

−1

0

0

false positives per window (FPPW )

false positive rate

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

0

0

0.5

1

1.5

2

2.5

3

3.5

4

4.5

5

false positives per image (FPPI)

false positives per image (FPPI)

false positives per image (FPPI)

Example True Detections

(a)

(b)

(c)

(d)

(e)

Figure 6. Evaluation of our method on multiple pedestrian datasets. First row shows performance and comparisons with state-of-the-art methods. Second row shows some sample true detections for each dataset (best visualized in color).

reported results. The PLS approach outperforms all methods in regions of low false alarm rates, i.e. 5.8% miss rate at 10−5 FPPW and 7.9% miss rate at 10−6 FPPW. DaimlerChrysler Pedestrian Dataset. This dataset provides grayscale samples of size 18 × 36 pixels [18]. We adapt our feature extraction methods for these image characteristics as follows. For co-occurrence feature extraction, we use block sizes of 8 × 8 and 16 × 16 with shifts of 2 pixels for both. Co-occurrence matrices are estimated using the brightness channel quantized into 16 bins. For HOG feature extraction, we adopt the same approach used for the INRIA person dataset; however, block sizes now range from 8 × 8 to 18 × 36. Due to the lack of color information, the color frequency feature cannot be considered in this experiment. The DaimlerChrysler dataset is composed of ﬁve disjoint sets, three for training and two for testing. To obtain results that can be compared to those presented by Maji et al. [14] and by Munder and Gavrila [18], we report results by training on two out of three training sets at a time. Therefore, we obtain six curves from which the conﬁdence interval of the true mean detection rate is given by the t(α/2,N −1) distribution with desired conﬁdence of 1 − α = 0.95 and N = 6. The boundaries of this interval are approximated by y ± 1.05s, where y and s denote the estimated mean and standard deviation, respectively [18]. Figure 6(b) compares results obtained by the proposed method to results reported in [14, 18]. In contrast to previous graphs, this shows detection rates instead of miss rates on the y-axis and both axes are shown using linear scales. Similar to experiments conducted on the INRIA person dataset, the results obtained with the proposed method show improvements in regions of low false alarm rates. ETHZ Dataset. We evaluate our method for un-cropped full images using the ETHZ dataset [7]. This dataset provides four video sequences, one for training and three for testing (640×480 pixels at 15 frames/second). Even though a training sequence is provided, we do not to use it; instead we use the same PLS model and QDA parameters learned on the INRIA training dataset. This allows us to evalu-

ate the generalization capability of our method to different datasets. For this dataset we use false positives per image (FPPI) as the evaluation metric, which is more suitable for evaluating the performance on full images [21]. Using the same evaluation procedure described in [7] we obtain the results shown in Figure 6(c), (d) and (e) for the testing sequences provided. We use only the images provided by the left camera and perform the detection for each single image at 11 scales without considering any temporal smoothing. We do not train our detector on the provided training set and we do not use any additional cues such as depth maps, ground-plane estimation, and occlusion reasoning, all of which are used by [7]. Yet, our detector outperforms the results achieved by [7] in all three video sequences. The work by Ess et al. [8] also considers sequence #1 in their experiments, so we have added their results in Figure 6(c). Even though [8] uses additional cues such as tracking information, our method, trained using the training set of INRIA dataset, achieves very similar detection results. Additional Set of Images. We present some results in Figure 7 for a few images obtained from INRIA testing dataset and Google. These results were also obtained using the same PLS model and QDA parameters learned on the INRIA training dataset. We scan each image at 10 scales. Despite the large number of detection windows considered, the number of false alarms produced is very low.

5. Conclusions
We have proposed a human detection method using a richer descriptor set including edge-based features, texture measures and color information, obtaining a signiﬁcant improvement in results. The augmentation of these features generates a very high dimensional space where classical machine learning methods are intractable. The characteristics of our data make an ideal setting for applying PLS to obtain a much lower dimensional subspace where we use simple and efﬁcient classiﬁers. We have tested our approach

(a) 640 × 480 (41,528 det. windows)

(b) 1632 × 1224 (389,350 det. windows)

(c) 1600 × 1200 (373,725 det. windows)

Figure 7. Results obtained from images containing people of different sizes and backgrounds rich in edge information. The image size and the total number of detection windows considered are indicated in the caption (best visualized in color).

on a number of varied datasets, demonstrated its good generalization capabilities and shown it to outperform state-ofthe-art methods that use additional cues.

Acknowledgements
This research was partially supported by the ONR MURI grant N00014-08-10638 and the ONR surveillance grant N00014-09-10044. W. R. Schwartz acknowledges “Coordenacao de Aperfeicoamento de Pessoal de N´vel Su¸˜ ¸ ı perior” (CAPES - Brazil, grant BEX1673/04-1). The authors also thank Ryan Farrell for his useful comments.

References
[1] J. Begard, N. Allezard, and P. Sayd. Real-time human detection in urban scenes: Local descriptors and classiﬁers selection with adaboost-like algorithms. In CVPR Workshops, 2008. [2] S. Belongie, J. Malik, and J. Puzicha. Matching Shapes. In ICCV 2001, volume 1, pages 454–461 vol.1, 2001. [3] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at www.csie.ntu.edu.tw/ cjlin/libsvm. [4] Y.-T. Chen and C.-S. Chen. Fast human detection using a novel boosted cascading structure with meta stages. Image Processing, IEEE Trans. on, 17(8):1452–1464, 2008. [5] N. Dalal and B. Triggs. Histograms of Oriented Gradients for Human Detection. In CVPR 2005, 2005. [6] P. Dollar, B. Babenko, S. Belongie, P. Perona, and Z. Tu. Multiple Component Learning for Object Detection. In ECCV 2008, pages 211–224, 2008. [7] A. Ess, B. Leibe, and L. V. Gool. Depth and appearance for mobile scene analysis. In ICCV, October 2007. [8] A. Ess, B. Leibe, K. Schindler, and L. Gool. A mobile vision system for robust multi-person tracking. CVPR, 2008. [9] P. Felzenszwalb, D. McAllester, and D. Ramanan. A discriminatively trained, multiscale, deformable part model. CVPR, pages 1–8, June 2008. [10] R. Haralick, K. Shanmugam, and I. Dinstein. Texture Features for Image Classiﬁcation. IEEE Transactions on Systems, Man, and Cybernetics, 3(6), 1973. [11] Y. Ke and R. Sukthankar. PCA-SIFT: A More Distinctive Representation for Local Image Descriptors. In CVPR 2004, volume 2, pages 506–513, 2004.

[12] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR 2006, pages 2169–2178, 2006. [13] Z. Lin and L. S. Davis. A pose-invariant descriptor for human detection and segmentation. In ECCV, 2008. [14] S. Maji, A. Berg, and J. Malik. Classiﬁcation using intersection kernel support vector machines is efﬁcient. In CVPR, June 2008. [15] K. Mikolajczyk, C. Schmid, and A. Zisserman. Human detection based on a probabilistic assembly of robust part detectors. In ECCV 2004, volume I, pages 69–81, 2004. [16] A. Mohan, C. Papageorgiou, and T. Poggio. Examplebased object detection in images by components. PAMI, 23(4):349–361, 2001. [17] Y. Mu, S. Yan, Y. Liu, T. Huang, and B. Zhou. Discriminative local binary patterns for human detection in personal album. In CVPR 2008, pages 1–8, June 2008. [18] S. Munder and D. Gavrila. An experimental study on pedestrian classiﬁcation. PAMI, 28(11):1863–1868, 2006. [19] R. Rosipal and N. Kramer. Overview and recent advances in partial least squares. Lecture Notes in Computer Science, 3940:34–51, 2006. [20] V. Shet, J. Neuman, V. Ramesh, and L. Davis. Bilattice-based logical reasoning for human detection. In CVPR, 2007. [21] D. Tran and D. Forsyth. Conﬁguration estimates improve pedestrian ﬁnding. In NIPS 2007, pages 1529–1536. MIT Press, Cambridge, MA, 2008. [22] O. Tuzel, F. Porikli, and P. Meer. Human detection via classiﬁcation on riemannian manifolds. In CVPR, 2007. [23] H. Wold. Partial least squares. In S. Kotz and N. Johnson, editors, Encyclopedia of Statistical Sciences, volume 6, pages 581–591. Wiley, New York, 1985. [24] S. Wold, W. Johansson, and M. Cocchi. PLS - Partial LeastSquares Projections to Latent Structures. In H. Kubinyi, editor, 3D QSAR in Drug Design: Volume 1: Theory Methods and Applications, pages 523–550. Springer Verlag, 1993. [25] B. Wu and R. Nevatia. Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors. In ICCV, pages 90–97, 2005. [26] B. Wu and R. Nevatia. Optimizing discrimination-efﬁciency tradeoff in integrating heterogeneous local features for object detection. In CVPR 2008, pages 1–8, June 2008. [27] W. Zhang, G. Zelinsky, and D. Samaras. Real-time accurate object detection using multiple resolutions. In ICCV, 2007. [28] Q. Zhu, M.-C. Yeh, K.-T. Cheng, and S. Avidan. Fast human detection using a cascade of histograms of oriented gradients. In CVPR 2006, pages 1491–1498, 2006.

Incremental Multiple Kernel Learning for Object Recognition
Aniruddha Kembhavi† , Behjat Siddiquie† , Roland Miezianko§ , Scott McCloskey§ , Larry S. Davis† † University of Maryland, College Park § Honeywell Labs
∗

Abstract
A good training dataset, representative of the test images expected in a given application, is critical for ensuring good performance of a visual categorization system. Obtaining task speciﬁc datasets of visual categories is, however, far more tedious than obtaining a generic dataset of the same classes. We propose an Incremental Multiple Kernel Learning (IMKL) approach to object recognition that initializes on a generic training database and then tunes itself to the classiﬁcation task at hand. Our system simultaneously updates the training dataset as well as the weights used to combine multiple information sources. We demonstrate our system on a vehicle classiﬁcation problem in a video stream overlooking a trafﬁc intersection. Our system updates itself with images of vehicles in poses more commonly observed in the scene, as well as with image patches of the background, leading to an increase in performance. A considerable change in the kernel combination weights is observed as the system gathers scene speciﬁc training data over time. The system is also seen to adapt itself to the illumination change in the scene as day transitions to night.
I MKL O b ject D et ect or

Figure 1. Sample result frames showing varying illumination conditions. Our incremental framework (IMKL) tunes itself to the scene by updating itself with images of objects in commonly observed poses and images of the varying background. Thus, it outperforms a static detector built on a generic training set.

1. Introduction
The problem of visual category recognition has received considerable interest over the past few years. The most common approach consists of three major components: interest point detection, interest region description and classiﬁcation. A recent focus has been on improving region descriptors. This has led to a number of powerful descriptors being proposed such as Histograms of Oriented Gradients [8], Geometric Blur [3] and Pyramidal Histogram of Visual Words [6]. While each of these descriptors provides good classiﬁcation accuracies for different object classiﬁcation tasks, combining information from such multiple sources has been shown to be more reliable [5, 24, 19]. Varma et al. [19] proposed combining multiple descriptors using Multiple Kernel Learning (MKL) and showed impressive results on varied object classiﬁcation tasks. Using such a set of powerful descriptors, along with a nonlinear classiﬁer such as a Support Vector Machine (SVM), can lead to a boost in classiﬁcation performance.
∗ Corresponding

author: anikem@umd.edu

But it is equally important to have a good set of training images, representative of the test images that are expected in the given application. Collecting large number of images and forming a generic training dataset for commonly seen objects is relatively easy using an internet search engine such as Google. Furthermore for many standard objects such as cars, training datasets are already available, such as the UIUC Car Database [1]. However, obtaining a representative training database for a given application is not as straightforward, as it requires a fair amount of manual labor. Consider a camera at a trafﬁc intersection detecting and classifying vehicles such as shown in Figure 1. First, the location of the camera in this scene and typical paths traversed by the vehicles, restricts the observed poses. Second, the camera position restricts the images representing the negative class (in our case, the background images) for this classiﬁcation task. Third, images corresponding to vehicles as well as background also change over time, due to factors such as illumination changes and shadows cast by the nearby buildings. Obtaining such scene speciﬁc exam1

Gen er ic O b ject D et ect or

ples of the object classes and the background class would clearly beneﬁt the visual classiﬁer, but would require a tedious manual annotation procedure. Our Incremental Multiple Kernel Learning (IMKL) approach uses an easily obtained generic training database as input, and then tunes itself to the classiﬁcation task at hand. It simultaneously updates the training examples to tailor them towards the objects in the scene. It also updates the weights that determine the optimal combination of different information sources1 , while allowing different combinations to be chosen for different object classes. Finally, it tunes the classiﬁer to the updated training dataset. As the scene changes over time, a feedback loop updates our training dataset with detections from all object classes. The incremental procedure is then invoked to update the kernel combination weights as well as the classiﬁer. Our ﬁnal system is obtained by combining the outputs of this online classiﬁer with the high probability outputs of the original ofﬂine classiﬁer trained on the generic training database. This enables us to tune the classiﬁer to the given scene, while reducing the number of misclassiﬁcations on rarely seen objects. We can also remove images from our training database over time. This is useful when dealing with gradual illumination changes, for example. We ﬁrst describe the MKL formulation of Bach et al. [15], known as SimpleMKL, which we use to obtain a classiﬁer for the initial training database. SimpleMKL carries out this optimization in an SVM framework to simultaneously learn the SVM model parameters as well as kernel combination weights. Our incremental procedure for MKL is an exact online solution that allows us to update the Lagrangian multipliers of the training points, as well as the kernel combination weights, one new point at a time. The central idea is to add a new data point to the solution and update its Lagrangian multiplier while maintaining the Karush-Kuhn-Tucker conditions on all the current data points in the solution. We derive our IMKL procedure in Section 3. We demonstrate our visual categorization framework on the task of vehicle detection and classiﬁcation. The dataset we use consists of video sequences collected from a camera overlooking a trafﬁc intersection. We initialize our training database with a set of images collected from Google and update it incrementally to improve the classiﬁcation performance over time. The dataset also shows a signiﬁcant change in illumination conditions in the scene as day transitions into night. Our system is able to update itself over time to handle this transition. We compare our algorithm with OPTIMOL [13], an incremental model learning approach, recently proposed for the task of automatic object dataset collection.

2. Related Work
Early works on object recognition used global features such as color or texture histograms [14]. However these
1 In

this paper each information source refers to a kernel matrix.

features were not robust to view-point changes, clutter and occlusion. Over the years, more sophisticated approaches such as part-based [9] and bag-of-features [16] methods have become more popular. Increased interest in object recognition has resulted in new feature descriptors and a multitude of classiﬁers. Inspired by the pyramidal feature matching approach of [12], Bosch et al. proposed two new region descriptors - the Pyramidal Histogram of Oriented Gradients (PHOG) and Pyramidal Histogram of Visual Words (PHOW) [6]. These features were then used with Random Forests as a multiway classiﬁer [5]. Zhang et al. used the Geometric Blur (GB) feature [3] and proposed using a discriminative nearest neighbor classiﬁcation for object recognition [23]. Wu et al. [21] used edgelet features to capture the local shape of objects and were able to simultaneously detect and segment objects of a known category. Zhang et al. [24] combined multiple descriptors and obtained improved results for texture classiﬁcation and object recognition. They provided equal weights to each descriptor. Similarly, Bosch et al. [5] linearly combined the PHOG and PHOW descriptors to obtain improved performance. The linear combination weights were, however, obtained by a brute force search using a validation dataset. Since the number of features was small, their search space had few dimensions, thus making the brute force computationally feasible. Wu et al. [22] combined multiple heterogeneous features for object detection by using cascade structured detectors in a boosting framework. Features were combined using their classiﬁcation powers and computational cost. Lanckriet et al. [11] introduced the MKL procedure to learn a set of linear combination weights, while using multiple sources of information with a kernel method, such as an SVM. Their problem formulation, however, resulted in a convex but non-smooth minimization problem. Bach et al. [2] considered a smoothed version of the problem. Their Sequential Minimal Optimization (SMO) algorithm was signiﬁcantly more efﬁcient than the previous formulation in [11]. Sonnenburg et al. [17] reformulated the problem as a semi-inﬁnite linear program and solved it efﬁciently by recycling the standard fast SVM implementations. Their algorithm worked for hundreds of thousands of examples or hundreds of kernels. Rakotomamonjy et al. [15] formulated the problem using a 2-norm regularization formulation to a smooth and convex optimization problem. Their method provided the additional advantage of encouraging sparse kernel combinations. Varma et al. [19] combined multiple features using MKL and showed a considerable increase in the performance of their visual classiﬁer. A number of unsupervised, online learning algorithms have been used for computer vision applications. Li et al. [13] used a non-parametric graphical model in an incremental approach for automatic dataset collection from the Internet (OPTIMOL). Their iterative framework simultaneously learns object category models and collects object category datasets. We compare our IMKL method with OP-

TIMOL in Section 5. Boosting techniques for incremental learning have also been popular. Javed et al. [10] used co-training to label incoming data and used it to update a boosted classiﬁer. Co-training [4] is a method for training a pair of learners, given that the two algorithms use different views of the data. The two classiﬁers are used to provide additional informative labeled examples to one another, which improves the overall performance. Wu et al. [20] extended the online boosting algorithm and proposed an online framework for cascade structured detectors. An automatic labeler called the oracle, with a high precision rate, provided samples to update the online object detector. In order to prevent the boosting algorithm from overﬁtting noisy data (provided by the oracle), they employed two noise resistant strategies from variants of the Adaboost algorithm designed to be robust to outliers. Our initial object classiﬁer, built from a generic training dataset, is tuned similar to this oracle. Our work builds on MKL and ﬁts well into the SVM framework. It also provides the useful property of being able to adapt kernel weights over time in addition to updating the training database.

standard SVM solver and determining the kernel combination weights using a projected gradient descent method.

3.2. Karush-Kuhn-Tucker Conditions
The support vectors returned by the training algorithm of an SVM generally represent a small fraction of all the training examples, but are able to summarize the decision boundary between the classes very well. Thus, one way to increment an SVM is to retain only the support vectors, to reduce the computational load required at every successive training step [18]. The same approach could be used for the MKL problem. However, this gives only approximate results. The ﬁrst exact online approach to train SVMs was proposed by Cauwenberghs et al. [7]. New data points are presented to the SVM one at a time. The new data point is added to the solution while ensuring that the Karush-KuhnTucker (KKT) conditions are retained on all the previous data points. Our proposed approach to IMKL is inspired by this work. The key idea behind the Incremental SVM is that the SVM optimization problem is convex. Thus, the KKT conditions are not only necessary but also sufﬁcient. Thus, maintaining the KKT conditions on all old points, as well as the new point, indicates that a new solution has been obtained. The optimization problem given by the SimpleMKL framework in Equation 2 is also convex, making it suitable for our purposes. The KKT conditions for our problem are derived from the Lagrangian function corresponding to Equation 2,
L= 1 2 wk wk +C dk ξi −
i i

3. An Incremental Solution
3.1. The Multiple Kernel Learning Problem
Kernel based learning methods have proven to be an extremely effective discriminative approach to classiﬁcation as well as regression problems. Given multiple sources of information, one might calculate multiple basis kernels, one for each source. In such cases, the resultant kernel is often computed as a convex combination of the basis kernels,
K K

Φ(xi , xj ) =
k=1

dk Φk (xi , xj ),
k=1

dk = 1 , dk ≥ 0

(1)

νi ξi − µk dk − dk − 1)
k

k

where xi are the data points, Φk (xi , xj ) is the k th kernel and dk are the weights given to each information source (kernel). Learning the classiﬁer model parameters and the kernel combination weights in a single optimization problem is known as the Multiple Kernel Learning problem [11]. There have been a number of formulations for the MKL problem, as noted in Section 2. Our incremental approach builds on the MKL formulation of [15], known as SimpleMKL. This formulation enables the kernel combination weights to be learnt within the SVM framework. The optimization equation is given by,
min
k

αi (yi wk φk (xi ) + yi b − 1 + ξi ) − λ(
i

(3)

1 T wk wk + C dk

ξi
i

where αi is the Lagrange multiplier corresponding to the ﬁrst constraint in Equation 2, νi and µk are the Lagrange multipliers associated with the non-negativity constraints on ξi and dk respectively, while λ corresponds to the Lagrange multiplier of the l1 -norm equality constraint on d. The optimal solution of the multiple kernel system in Equation 2 occurs at the saddle point of Equation 3. The saddle point is obtained by differentiating the Lagrangian equation with respect to the primal variables (wk , dk , ξi , b) and the dual variables (αi , νi , µk ). A small amount of algebraic manipulations yields the KKT conditions given below,
gi =
j k

such that yi
k

φk (xi ) + yi b ≥ 1 − ξi ∀i dk = 1
k

(2) 1 2

dk αj Qk + yi b − 1 = 0, ij
i

αi yi = 0 dk = 1
k

ξi ≥ 0 ∀i, dk ≥ 0 ∀k,

αi αj Qk + µk − λ = 0, µk dk = 0, ij
i j

(4)

where b is the bias, ξi is the slack afforded to each data point and C is the regularization parameter. The solution to the above MKL formulation is based on a gradient descent on the SVM objective value. An iterative method alternates between determining the SVM model parameters using a

where Qk = yi Φk (xi , xj )yj . ij Note that gi = yi f (xi ) − 1, where f (xi ) is the solution of the multiple kernel SVM given by,
f (xnew ) =
j k

dk αj yj Φk (xj , xnew ) + b

(5)

gi > 0

3.3. Algorithm
Consider a set of data instances (x1 , x2 , . . . , xn ) with corresponding class labels (y1 , y2 , . . . , yn ). Let Φk (xi , xj ) be the set of K kernels. The MKL solution for the given data is obtained by SimpleMKL and it thus satisﬁes the KKT conditions in Equation 4. The data points are divided into three disjoint sets based on their Lagrange multipliers (αi s): set L containing the set of points lying on the correct side of the margin vectors (αi = 0), set S containing the support vectors (0 < αi < C) and set E containing the points lying on the wrong side of the margins (αi = C). We also divide the kernels into two sets: set D+ containing kernels with positive weights and set D0 with kernels having zero weight. These sets are illustrated in Figure 2. When a new point xq is added to the solution, we need to calculate its Lagrange multiplier αq (0 ≤ αq ≤ C) such that the KKT conditions are satisﬁed once again. We begin with a value αq = 0 and keep increasing it until we reach the updated solution. Every time we increment αq , the remaining Lagrangian multipliers, the kernel weights and the bias must be changed to maintain the constraints in Equation 4. These changes are given by the differential form of the constraints,

L gi(+,0) αi(+,0) S gi(0,-) αi(+,C) E

dk > 0

αi = 0 0 < αi < C

D+

μk = 0

gi = 0

dk(+,0) dk = 0

μk(+,0) μk > 0

gi < 0

D0

Figure 2. Categorization of the data points and kernels. The image on the left shows the values of the Lagrange multipliers (α s) and the output of the system (g s) for each of the sets: L, S and E. It also shows the conditions that are checked to detect a set transition. (Notation: gi (+, 0) denotes the value of gi changing from a positive value to 0.) The image on the right shows the two kernel sets, the corresponding values of the weights (d s) and their Lagrange multipliers (µ s) and the set change conditions.

• αq = C and gq < 0: xq is on the wrong side of the margin. Added to set E.

A similar procedure can be used for removing data points from the classiﬁer (decremental unlearning). k k The number of computations required by the IMKL alαj ∆dk Qij + ∆αj Qij gorithm depends on the computations to solve the nonj j k k linear system and the number of steps taken to reach the k + ∆dk ∆αj Qij + yi ∆b = 0, ∀i ∈ S, ∀j ∈ {S, E, L, q} ﬁnal value of ∆αq . In our experiments, we have observed j k that setting the initial solution of the non-linear solver to 1 a zero vector, reduces the computational cost signiﬁcantly. ∆αi ∆αj Qk ∆αi αj Qk + ij ij 2 i j The number of steps taken to reach the ﬁnal solution is i j lower bounded by the number of set changes that are re(6) quired to arrive at the ﬁnal solution. We use a large step +∆µk − ∆λ = 0, ∀k ∈ K size at every time instant and backtrack our solution if we αi yi = 0∀i ∈ {S, E, L, q}, ∆dk = 0 observe a set change for the given step size. The IMKL i k algorithm can also be sped up by ignoring the higher order ∆µk dk + µk ∆dk + ∆µk ∆dk = 0, ∀k ∈ K terms in Equation 6 to obtain linear equations. However this provides only an approximate solution. For a given step size ∆αq , Equation 6 is a set of Consider the two class classiﬁcation problem shown in (numS + 2K + 2) equations in (numS + 2K + 2) Figure 3. A new point q, marked in red, is added to the unknowns. Here, numS is the number of points in set S system, and it initially gets misclassiﬁed. As the Lagrange and K is the number of kernels. The unknown variables are: multiplier αq is incremented upwards from a value of 0, {∆α1 . . . , ∆αnumS , ∆d1 , . . . , ∆dK , ∆µ1 , . . . , ∆µK , ∆b, ∆λ}. the distance between the new point and the margin reduces, These non-linear equations can be solved using a standard while some of the other points change set membership. At non-linear equation solving package. Since an addition of the same time, the kernel combination weights also change. a new point may not alter the system signiﬁcantly, a good initial solution for all the unknowns in Equation 6 is 0. 4. Object Recognition Framework The above differential equations only hold when ∆αq is small enough to ensure that there is no change in set memA training database, representative of the expected test bership for either the points or the kernels. Thus, when set points, is an essential component of any classiﬁcation sysmembership changes, the differential equations are updated tem. In a practical object recognition framework, a good and the process is repeated. The conditions for a change in training database is one that contains images of the expected the set membership are described in Figure 2. objects in their more likely poses and illumination condiThe algorithm is terminated when any of the following tions. It must also contain a representative set of images in conditions occur. the negative set, which, in an object recognition framework, is usually the background. Obtaining such a set of good • gq > 0 at αq = 0: xq is a correctly classiﬁed point. training examples can often be a tedious process. On the Added to set L. other hand, it is easier to obtain a generic training dataset of • gq = 0 before αq = C: xq is a support vector. Added images of the expected object classes. Our object detector to set S.

αi = C

Figure 3. A 2-class classiﬁcation example. Points in class 1 are shown in orange and points in class 2 are shown in blue. Points in set S are marked with a black border. Points in set L are solid colored while points in set E are not ﬁlled with color. Kernel 1 (weight shown by the brown bar) captures the similarity between the y-coordinates of the points, while Kernel 2 (green bar) captures the similarity between the x-coordinates. The left ﬁgure shows the effect of adding a new point (shown in red) on the original points and the weights. A change in set membership is observed for some points. The ﬁgure on the right shows the ﬁnal classiﬁer after adding 7 new points close to the ﬁrst new point.

the local detector with false positives can lead to a signiﬁcant drop in the performance of the system, and the probability threshold is set sufﬁciently high to minimize this. On the other hand, for the background class, such an updating criterion leads to the addition of a large number of image patches from a single portion of the scene. This is because background patches with very similar appearances repeat over several frames. Thus if a patch gets classiﬁed with a very high probability of belonging to the negative set, several similar images also get added to the local training set. Ideally, one would like the entire scene to be well represented in the background class of the local detector. Thus, we ﬁrst threshold image windows classiﬁed by the global detector as belonging to the background class. Then, for every image patch passing this initial criterion, we evaluate its positional entropy with respect to the distribution of the positions of all image patches currently in the local background training set. This is given by, H(I) = − p(w(x,y) |I(x,y) ) log p(w(x,y) |I(x,y) )
w∈{BGlocal }

is initialized on a generic training dataset and tunes itself towards the objects and background in the scene.
Generic Training Database

Global Object Detector

Local Object Detector

Updating Criteria

Figure 4. Object recognition framework.

Figure 4 provides an overview of our visual categorization framework. Training images from a generic training dataset are used to train an initial object detector which we call the global detector. The global detector is not updated at any time and serves as a generic object classiﬁer. The generic training dataset is also used to train a local object detector, which runs in an online mode throughout the duration of analysis. Incoming images from a video stream are scanned using overlapping windows and each window is classiﬁed into one of the classes by both the detectors. The classiﬁcation results returned by the global detector keep updating the training image sets of the local object detector. The updating criterion differs for the foreground classes (buses and cars) and the background class. The image windows that are classiﬁed by the global detector as belonging to one of the foreground classes are thresholded so as to retain only very high conﬁdence detections. Such windows are considered reliable detections and used to update the foreground training sets of the local object detector. Since the purpose of the local object detector is to train on typically observed appearances and poses, updating it with high conﬁdence samples works well. The high precision of the global detector comes at the cost of a lower recall. Updating

(7) where w represents an image patch in the current background set, I represents the new image patch and (x, y) represent the co-ordinates of an image patch in the scene. Image patches passing the initial background threshold, as well as having a high entropy with respect to the current local training set, form good candidates to improve the diversity of the local background set and are added to it. Over time, the object classes get updated with images of objects in Updating their typical observed appearances and poses and the criteria 2 background class gets updated with image patches from different parts of the scene. Figure 5 demonstrates the image patches in the local background set which has been updated using both criteria. Using the entropy criteria in addition to a probability threshold, samples the entire scene well. Li et al. [13] used a similar criteria to update their dataset. While their entropy is calculated in the feature space, our measure is calculated in the image co-ordinate space. The local detector ﬁts itself towards image patches observed in the recent past, improving its performance. However, it also has the tendency of misclassifying objects that are atypical in the scene, due to overﬁtting on the observed data. The more generically trained global detector helps classify such atypical objects. The outputs of both detectors are combined to obtain the ﬁnal detections. The resultant object detections are used to update the local detector. In order to ﬁt the local detector towards a dynamically changing scene, it is also important to discard image patches from the local training dataset. For every image patch added to the local set, we retain a timestamp indicating the frame it was obtained from. We use this to discard training samples based on the length of their stay in the training set. Thus the classiﬁer adapts itself towards changing illumination conditions, particularly when day transitions to night. Our IMKL algorithm described in Section 3 is used to update the Local classiﬁer with new training images. This

Buses
Initial Training Set

Cars

Background

Figure 5. Representation of the local negative training set using two sampling methods to update the training set. For this display, all image patches in the set are added together at the appropriate locations in the scene. Thus brighter regions corresponds to more patches in that portion of the scene, black regions indicate that no image patches represent that portion of the scene. (Left) High probability criteria - Only certain portions of the scene are represented. (Right) High probability + high entropy criteria - Most portions of the scene are represented equally.

Figure 6. Snapshots of the training set at 4 time instants. Top row shows the initial training set. The next 3 rows show sample images added to local over time. The illumination change is noticeable at each time instant. The dataset gets updated with many objects in similar poses and representative background patches.

also results in an update of the kernel combination weights based on the training data. We use multiple 1-Vs-All classiﬁers for our purpose of multi-class classiﬁcation. This enables us to compute a separate set of kernel combination weights, one for each object class. In Section 5 we show an example of the evolution of these kernel weights over time.

5. Experiments
We test the performance of our system on the task of object detection on videos taken from a trafﬁc dataset. This dataset consists of 11 challenging videos (480 x 704 pixels at 15 frames/second), of a busy intersection, taken from a trafﬁc surveillance camera. The total number of frames is more than 120,000. Our task is to detect two classes of objects, cars and buses. We have ground truth marked for every tenth frame in this dataset. Due to the camera location and trafﬁc restrictions in the scene, cars in the video typically have a frontal view, while buses typically appear in a proﬁle view. Other views are also observed, but they are less common. The car category includes cars of varying sizes as well as SUV’s and trucks. With a few exceptions, buses have a similar appearance, since most of them are public transportation buses. The dataset consists of videos captured at different times of the day, resulting in a variety of illumination conditions as shown in Figure 1, including street-lights at night. For videos captured during the transition of day to night, the appearances of the vehicles also change (most prominently, vehicles in the dark have their headlights turned on).

at different levels are treated independently. Our IMKL algorithm automatically weights each level of the pyramid based on the training dataset. Histogram intersection is used as the similarity metric for all features in this paper. The ﬁrst feature gives rise to 4 kernels, one for each level of the pyramid. The second feature is the PHOG-360. It only differs from PHOG-180 in that orientations are calculated over the interval [0, 360]. This also gives rise to 4 kernels. The third feature, PHOW-Gray [6], encodes appearance. SIFT features are densely sampled at 10 pixel intervals in each direction and quantized to a 300 visual words vocabulary. Histograms of visual words are calculated over an increasing number of grids at each pyramidal level. We use 3 levels. The fourth feature is PHOW-Color. The only difference from PHOW-Gray is that it is calculated on the 3 channels of the HSV image. These give rise to 6 kernels. The ﬁfth feature is Geometric Blur (GB) [3], which captures shape information of the objects and also accounts for the geometric distortion between images. The un-quantized GB feature was used with an expensive correspondence based distance metric in [23]. However, in order to speed-up computations, we quantized the GB feature to a set of 300 visual words. We then calculated histograms of GB words in the same pyramidal framework to enforce some measure of spatial constraints. We used a 3 level pyramid. Thus we obtained a total of 17 kernel matrices.

5.2. Analysis
Evaluation of MKL. We ﬁrst evaluate the power of using multiple kernels and using MKL to determine kernel weights for the given classiﬁcation task. For this purpose, we created a validation dataset consisting of images of buses, cars and background extracted from the ground truth as well as the initial training set (obtained from Google). We then individually evaluated each kernel as well as the combination of kernels using a Sum of Kernels (SoK) approach (such as in [12]) and an MKL approach for both object classes over the validation set. The SoK approach assigns equal weights to all kernels. SoK has been known to provide good results when kernels are carefully chosen for the given data, but its performance degrades in the presence

5.1. Kernel Matrices
We use 5 kinds of features in our system, giving rise to a total of 17 kernel matrices. The ﬁrst feature used is the Pyramidal Histogram of Oriented Gradients (PHOG-180) [6] to represent local shape. This consists of HOG features calculated over increasingly ﬁner spatial grids. The orientations are calculated over the interval [0, 180].We set the number of levels of the pyramid to 4. HOG features calculated for grids within the same level of the pyramid are concatenated to form a long feature vector, but feature vectors calculated

Sample images added to Local

1

1 0.9 0.8 0.7

1 0.9 0.8 0.7

Processing time (secs)

Sum of Kernels PHOW−Color Geometric Blur Multiple Kernel Learning

0.9 0.8 0.7

Generic Object Detector Updated Obj Detector − SoK Updated Obj Detector − OPTIMOL Updated Obj Detector − IMKL

6

Retraining with MKL Updating with IMKL
5

Precision

Precision

0.6 0.5 0.4 0.3 0.2 0.1 0 0

0.6 0.5 0.4 0.3

4

Precision

0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.2 0.4 0.6 0.8 1

3

2

Generic Object Detector Updated Obj Detector − SoK Updated Obj Detector − OPTIMOL Updated Obj Detector − IMKL
0.2 0.4

0.2 0.1 0.8 1 0 0 0.2 0.4 0.6 0.8 1

1

Recall

Recall

0.6

0

Recall

0

50

100

150

200

250

300

Number of increments

(a) Evaluation of MKL

(b) Precision-Recall for buses

(c) Precision-Recall for cars

(d) Efﬁciency of IMKL

Figure 7. (a) shows the evaluation of the individual kernels, combination using SoK and combination using MKL. MKL outperforms all other schemes. The best performing individual kernel is GB. (b) and (c) show the Precision-Recall curves for the bus and car classes respectively. Using our incremental object detector consistently increases performance in both cases. (d) compares the processing time of our incremental approach to retraining the MKL system at every step using all available images.

of noisy kernels. In our experiments, the MKL approach performs better than all other methods where as the SoK approach comes in second, outperforming both GB (the best performing individual feature) and the popular SIFT feature. Figure 7(a) shows the results for the Buses class. Local dataset snapshots. We now demonstrate results of our IMKL approach on the video dataset. Starting from a generic training dataset, our IMKL algorithm simultaneously updates the training dataset as well as the kernel combination weights. Figure 6 shows snapshots of the training database at different time instants for one video. Kernel weights over time. Figure 8 demonstrates the change of kernel combination weights over time. For this experiment, we chose a video where the scene is bright in the beginning but gets very dark by the end. We do not display kernel weights 1 to 8, since they do not show considerable change over time. Time 1 refers to the initial training dataset obtained from Google. Between times 1 and 2, we do not update the foreground classes to study the effect of updating only the background training set. This also causes a non-trivial change of weights (Time 2). After time 2, we update all object classes. Between times 2 and 3, the scene is bright. In this period, the detector tunes itself towards objects of speciﬁc poses and background patches. Beyond time 3, the scene gets darker. Here, PHOW-Color weights show a considerable drop (kernels 12-14), since color information in the video deteriorates, while PHOW-Gray kernels get higher weights. GB at ﬁne spatial resolution (kernel 17) gets high weights with decreasing illumination, indicating added importance to positional information (such as importance given to the position of vehicle headlights). Performance evaluation. Figures 7(b) and 7(c) show the performance of our system for the bus and car classes respectively, averaged over all videos in the dataset. We compare our IMKL object detector with 3 other detectors. Our baseline detector (which we call the Generic detector), represents an object detector built ofﬂine using only the generic training dataset and is not updated over time. It uses all 17 kernels and MKL to obtain the kernel weights. Our second comparison is to an object detector built on the generic

Illumination conditions
2 - Bright scene 3 - Bright scene 4 - Darker scene 5 - Dark scene
0.5

Feature Weights

0.4 0.3 0.2 0.1

l Set Initia ating Upd ound 1 gr Ti 2 back ating m Upd round eI 3 g ns back ground e ta 4 +for

nt s

G PHO
9

y -Gra
10

PHO
11

olor W-C
12 13

Geo
14

m-B
15

lur
16

0 17

5

res Featu

Figure 8. Kernel combination weights sampled at multiple time instants. Results shown for Buses class. (see text for details)

Google dataset and updated over time, but using SoK (equal kernel weights). Since these kernel weights are ﬁxed over time, an incremental SVM approach sufﬁces as the classiﬁer. Our third comparison is to OPTIMOL [13], an incremental model learning approach, recently proposed for automatic object dataset collection.2 The OPTIMOL algorithm is run independently of the IMKL system with a single change. In [13], Li et. al use SIFT as their feature descriptor. But given the superior performance of GB in our validation set (Figure 7(a)), we use histograms of GB based visual words as our feature descriptor for OPTIMOL. Our IMKL approach outperforms the other 3 methods, especially at high recalls. Figure 10 provides some more insight into the results. This plot shows the performance of the various methods over time for one of the videos in our dataset for which illumination changes. The images at the bottom show a sample frame within the speciﬁed time interval. OPTIMOL starts off slowly but as it gets updated, it catches up with the rest of the object detectors. As the scene gets darker, however, its performance deteriorates. OPTIMOL uses GB, and even our IMKL approach begins to reduce the importance given to this kernel when the scene be2 We

obtained code for OPTIMOL from the authors.

Figure 9. Sample results from a video sequence showing the ability of our system to adapt to gradual illumination changes.

comes dark. We also noticed a low overall performance of OPTIMOL (on a subset of the data) while using other kernels such as PHOW-Gray and PHOW-COLOR. This is because no single kernel has been able to provide consistently good results in all scene conditions. Using multiple kernels with ﬁxed weights (SoK) was also sub-optimal. Our IMKL approach provided the best results because it was able to dynamically change kernel weights based on the current object and scene characteristics. IMKL’s performance decreases at times 4 and 6 since the scene changes, but recovers at instants 5 and 7, once it updates itself sufﬁciently. Figure 9 shows sample results. Overall, we detect buses more reliably than cars. We are unable to consistently detect cars smaller than 60x60 pixels, which is the case for cars approaching from a distance, giving rise to a number of false negatives. Finally, Figure 7(d) illustrates the computational efﬁciency of the IMKL algorithm as compared to retraining the entire system using SimpleMKL.
Our approach (IMKL) Generic object detector
1

Updated detector - SoK Updated detector - OPTIMOL

0.9 0.8 0.7 0.6 0.5 0.4

Bright illumination
1 2 3

Golden illumination

Dark illumination
5 6 7

Time instants

4

Figure 10. Performance comparison of object detectors over time for a single video for Buses class. (see text for details)

6. Conclusion
We have proposed an Incremental Multiple Kernel Learning (IMKL) approach to object recognition and demonstrated the performance gains on a vehicle classiﬁcation task. Acknowledgements. This research was partially supported by the ONR surveillance grant N000140910044. The authors also thank Vlad Morariu for useful discussions.

References
[1] S. Agarwal, A. Awan, and D. Roth. Learning to detect objects in images via a sparse, part-based representation. IEEE Transactions on PAMI, 2004. [2] F. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the smo algorithm. ICML, 2004.

[3] A. Berg and J. Malik. Geometric blur for template matching. CVPR, 2001. [4] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. COLT: Proceedings of the Workshop on Computational Learning Theory, 1998. [5] A. Bosch, A. Zisserman, and X. Munoz. Image classiﬁcation using random forests and ferns. ICCV, 2007. [6] A. Bosch, A. Zisserman, and X. Munoz. Representing shape with a spatial pyramid kernel. CIVR, 2007. [7] G. Cauwenberghs and T. Poggio. Incremental and decremental support vector machine learning. NIPS, 2000. [8] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. CVPR, 05. [9] R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-invariant learning. CVPR03. [10] O. Javed, S. Ali, and M. Shah. Online detection and classiﬁcation of moving objects using progressively improving detectors. CVPR, 2005. [11] G. Lanckriet, N. Cristianini, L. El Ghaoui, P. Bartlett, and M. Jordan. Learning the kernel matrix with semi-deﬁnite programming. JMLR, 2004. [12] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. CVPR, 2006. [13] L. Li, G. Wang, and L. Fei-Fei. Optimol: automatic object picture collection via incremental model learning. CVPR, 07. [14] M. Pontil and A. Verri. Support vector machines for 3d object recognition. IEEE Transactions on PAMI, 1998. [15] A. Rakotomamonjy, F. R. Bach, S. Canu, and Y. Grandvalet. More efﬁcency in multiple kernel learning. ICML, 07. [16] J. Sivic, B. Russell, A. Efros, A. Zisserman, and W. Freeman. Discovering objects and location in images. ICCV, 2005. [17] S. Sonnenburg, G. R¨ tsch, C. Sch¨ fer, and B. Sch¨ lkopf. a a o Large scale multiple kernel learning. JMLR, 2006. [18] N. Syed, H. Liu, and K. Sung. Incremental learning with support vector machines. IJCAI, 1999. [19] M. Varma and D. Ray. Learning the discriminative powerinvariance trade-off. ICCV, 2007. [20] B. Wu and R. Nevatia. Improving part based object detection by unsupervised, online boosting. CVPR, 2007. [21] B. Wu and R. Nevatia. Simultaneous object detection and segmentation by boosting local shape feature based classiﬁer. CVPR, 2007. [22] B. Wu and R. Nevatia. Optimizing discrimination-efﬁciency tradeoff in integrating heterogeneous local features for object detection. CVPR, 2008. [23] H. Zhang, A. Berg, M. Maire, and J. Malik. Svm-knn: Discriminative nearest neighbor classiﬁcation for visual category recognition. CVPR, 2006. [24] J. Zhang, M. Marszalek, S. Lazebnik, and C. Schmid. Local features and kernels for classiﬁcation of texture and object categories: A comprehensive study. IJCV, 2007.

Area under ROC curve

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31, NO. XX,

XX 2009

1

Observing Human-Object Interactions: Using Spatial and Functional Compatibility for Recognition
Abhinav Gupta, Member, IEEE, Aniruddha Kembhavi, Member, IEEE, and Larry S. Davis, Fellow, IEEE
Abstract—Interpretation of images and videos containing humans interacting with different objects is a daunting task. It involves understanding scene/event, analyzing human movements, recognizing manipulable objects, and observing the effect of the human movement on those objects. While each of these perceptual tasks can be conducted independently, recognition rate improves when interactions between them are considered. Motivated by psychological studies of human perception, we present a Bayesian approach which integrates various perceptual tasks involved in understanding human-object interactions. Previous approaches to object and action recognition rely on static shape/appearance feature matching and motion analysis, respectively. Our approach goes beyond these traditional approaches and applies spatial and functional constraints on each of the perceptual elements for coherent semantic interpretation. Such constraints allow us to recognize objects and actions when the appearances are not discriminative enough. We also demonstrate the use of such constraints in recognition of actions from static images without using any motion information. Index Terms—Action recognition, object recognition, functional recognition.

Ç
1 INTRODUCTION
NDERSTANDING

human-object interactions require integrating various perceptual elements. We present a Bayesian approach for the interpretation of human-object interactions, that integrates information from perceptual tasks such as scene analysis, human motion/pose estimation,1 manipulable object detection, and “object reaction” determination.2 While each of these tasks can be conducted independently, recognition rates improve when we integrate information from different perceptual analysis and also consider spatial and functional constraints. Integrating information from different perceptual analyses enables us to form a coherent semantic interpretation of human-object interactions. Such an interpretation not
1. Recognition of action in static images is based on “implied” motion. “Implied” motion refers to the dynamic information implicit in the static image [26]. The inference of action from static images depends on implied motion, which itself depends on the phase of the action [27], [53]. This indicates that human pose provides important cues for action recognition in static images. 2. Object reaction is the effect of manipulation of an object by human actor.

U

. A. Gupta and L.S. Davis are with the Department of Computer Science, AV Williams Bldg., University of Maryland-College Park, College Park, MD 20742. E-mail: {agupta, lsd}@cs.umd.edu. . A. Kembhavi is with the Department of Electrical and Computer Engineering, 3364 AV Williams Bldg., University of Maryland-College Park, College Park, MD 20742. E-mail: anikem@umd.edu. Manuscript received 14 Aug. 2008; revised 22 Dec. 2008; accepted 7 Apr. 2009; published online 9 July 2009. Recommended for acceptance by Q. Ji, A. Torralba, T. Huang, E. Sudderth, and J. Luo. For information on obtaining reprints of this article, please send e-mail to: tpami@computer.org, and reference IEEECS Log Number TPAMISI-2008-08-0503. Digital Object Identifier no. 10.1109/TPAMI.2009.83.
0162-8828/09/$25.00 ß 2009 IEEE

only supports recognizing the interactions, but also the objects involved in those interactions and the effect of those interactions on those objects. Interactions between different perceptual analyses allow us to recognize actions and objects when appearances are not discriminative enough. Consider two objects, such as the spray bottle and a drinking bottle shown in Fig. 1. These objects are similar in appearance and shape, but have different functionality. Due to their functional dissimilarity, people’s interaction with these objects provides context for their recognition. Similarly, two similar human movements/ poses can serve different purposes depending on the context in which they occur. For example, the poses of the humans shown in Fig. 2 are similar, but, due to the difference in context, the first action is inferred to be running and the second action to be kicking. Another important element in the interpretation of human-object interactions is the effect of manipulation on objects. When interaction movements are too subtle to observe using computer vision, the effects of these movements can provide information on functional properties of the object. For example, when lighting a flashlight, recognizing the pressing of a button might be very difficult. However, the resulting illumination change can be used to infer the manipulation. We present two computational models for the interpretation of human-object interactions in videos and static images, respectively. Our approach combines action recognition and object recognition in an integrated framework, and allows us to apply spatial and functional constraints for recognition. The significance of our paper is threefold: 1) Human actions and object reactions are used to locate and recognize objects which might be difficult to locate or recognize otherwise. 2) Object context and object reactions are used to recognize
Published by the IEEE Computer Society

2

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. XX,

XX 2009

Fig. 2. Action recognition from static images requires contextual information. The same poses can have different meanings based on the context. (a) Running and (b) kicking. Fig. 1. Importance of interaction context in recognition of object and vice versa. While the objects might be difficult to recognize using shape features alone, when interaction context is applied the object is easy to recognize. Similarly, two actions might have similar dynamics and trajectories. It is difficult to differentiate between two actions based on the shape of trajectories. However, when cues from object are used in conjunction with cues from human dynamics, it is easy to differentiate between two actions.

actions which might otherwise be too similar to distinguish or too difficult to observe. In some cases, such as in recognition of actions from static images, there is no dynamic information; however, contextual information can be used in such cases for recognition. 3) We provide an approach for recognition of actions from “static” images. The extraction of “dynamic information” from static images has been well studied in the fields of psychology and neuroscience, but has not been investigated by the computer vision community.

2

RELATED WORK

did not lead to priming effects, priming was observed when humans were first asked to recognize the object and then recognize the image of a related hand gesture. In a recent study, Bach et al. [2] showed that when actions involving objects are perceived, spatial and functional relations provide context in which these actions are judged. These studies suggest that humans perceive implied motion from static poses under object and scene context. While most of this work suggests interactions between object and action perception in humans, they have not examined the nature of the interaction between action and object recognition. Vaina and Jaulent [54] address this through the study of pantomimes. They ranked the properties of objects that can be estimated robustly by perception of pantomimes of human-object interaction. They discovered that the weight of an object is most robustly estimated, while size and shape are harder to estimate.

2.1 Psychological Studies Our work is motivated by psychological studies of human information processing. With the discovery of mirror neurons in monkeys, there has been a renewed interest in studying the relationships between object recognition, action understanding, and action execution [38], [15], [16]. With the same neurons involved in execution and perception, a link between object recognition and action understanding has been established [38] in humans. Gallese et al. [15] showed that movement analysis in humans depends on the presence of objects. The cortical responses for goal directed actions are different from the responses evoked when the same action is executed but without the presence of the object. In another study, Frey et al. [25] showed that human inferior frontal cortex responds to static pictures of human-object interactions. The response was only observed in the presence of congruent poses and objects, suggesting that human poses are evaluated in the context of objects. On the other hand, the importance of action in perceiving and recognizing objects (especially manipulable objects like tools) has been shown [8]. Recent studies in experimental psychology have also confirmed the role of object recognition in action understanding and vice versa. Helbig et al. [23] show the role of action priming in object recognition and how recognition rates improve with action priming. Recognition rates of target objects were higher when the priming object was used in a similar action as the target object. In another study, Bub and Masson [7] investigated the role of object priming in static gesture recognition. While passive viewing of an object

2.2 Computational Approaches There has been a very large body of work carried out in both, object recognition and action recognition. Most approaches, however, address one or both of these problems, independent of the other. Computational approaches for object recognition typically use local static features, based on shape and textural appearance [9], [34], [21]. Berg and Malik [3] proposed the ‘geometric blur’ feature that is robust under affine distortions. Bosch et al. [6] proposed the Pyramidal Histogram of Oriented Gradients (PHOG) feature and the Pyramidal Histogram of Visual Words (PHOW) feature to represent local image shape and its spatial layout. Wu and Nevatia [58] proposed a set of silhouette-oriented features, called edgelet features, which were learned in a boosting framework to detect humans. Such approaches work well for detecting articulated/rigid objects, but encounter difficulties in recognizing manipulable objects due to the lack of discriminative power in these features. Todorovic and Ahuja [51] model object categories as characteristic configurations of parts that are themselves simpler subcategories, allowing them to cope better with nonrigid objects. However, like all appearance-based approaches, they still cannot deal with the many real-world objects that are similar in appearance but dissimilar in functionality. Functional properties of objects have also been used for object recognition. Functional capabilities of objects are derived from shape [45], [48], physics, and motion [11]. These approaches are limited by the lack of generic models that can map static shape to function. There has been recent interest in using contextual

GUPTA ET AL.: OBSERVING HUMAN-OBJECT INTERACTIONS: USING SPATIAL AND FUNCTIONAL COMPATIBILITY FOR RECOGNITION

3

information for object recognition. The performance of local recognition-based approaches can be improved by modeling object-object [35], [19] and object-scene relationships [49], [36]. Torralba and Sinha used low-level image cues [52] for providing context based on depth and viewpoint cues. Hoiem et al. [24] presented a unified approach for simultaneous estimation of object locations and scene geometry. Rabinovich et al. [43] proposed incorporating semantic object context as a postprocessing step to any object category recognition system using a conditional random field (CRF) framework. There are a wide range of approaches to human action recognition [46], [32], [22]. Analyzing human dynamics from image sequences of actions is a common theme to many of these approaches [5], [61], [44], [50]. While human dynamics provides important clues for action recognition, they are not sufficient for recognition of activities which involve action on objects. Many human actions involve similar movements/dynamics, but, due to their context sensitive nature, have different meanings. Vaina and Jaulent [54] suggested that action comprehension requires understanding the goal of an action. The properties necessary for achieving the goal were called Action Requirements and are related to the compatibility of an object with human movements such as grasps. Compared to the large body of work carried out in human action recognition from video sequences, there has been little work on recognition from single images. Wang et al. [56] presented an approach for discovery of action classes from static images using the shape of humans described by shape context histograms. Li et al. [29] tackled a different, but related, problem of event recognition from static images. They presented an approach to combine scene categorization and object recognition for performing event classification such as badminton and tennis. The problem of action recognition from static images is one level lower in the action hierarchy and corresponds to “verb” recognition in the hierarchy suggested by Nagel [37]. Attempts have been made before, to model the contextual relationship between object and action recognition. Wilson and Bobick [57] introduced parametric Hidden Markov Model (PHMM) for human action recognition. They indirectly model the effect of object properties on human actions. Davis et al. [10] presented an approach to estimate the weight of a bag carried by a person using cues from the dynamics of a walking person. Gupta et al. [17] presented an approach to estimate human pose using the contextual features from the objects being used in an activity. Moore et al. [33] conducted action recognition based on scene context derived from other objects in the scene. The scene context is also used to facilitate object recognition of new objects introduced in the scene. Kuniyoshi and Shimozaki [28] describe a neural network for the recognition of “true” actions. The requirements for a “true” action included spatial and temporal relationships between object and movement patterns. Peursum et al. [41] studied the problem of object recognition based on interactions. Regions in an image were classified as belonging to a particular object based on the relative position of the region to the human skeleton and the class of action being performed. All of the above work, models only one of the possible

interactions between two perceptual elements. Either they try to model the dependence of object recognition on human actions or vice versa. This assumes that one of the problems can be solved independent of the other, and the information from one can be used to aid in recognition of the other. Our previous preliminary work [18] modeled the twoway interactions between human actions and object perception. We presented a Bayesian model for simultaneous recognition of human actions and manipulable objects. Following our work, several recent papers have modeled the action-object cycle. Wu et al. [60] recognized activities based on detecting and analyzing the sequence of objects manipulated by the user, using a dynamic Bayesian network model. They combined information from RFID and video data to jointly infer the most likely activity and objects in the scene. Filipovych et al. [14] proposed a probabilistic graphical model of primitive actor-object interactions that combines information about the interactions’ dynamics, and actor-object static appearances and spatial configurations. However, none of these approaches can be easily extended to action recognition from static images.

3

VIDEO INTERPRETATION FRAMEWORK

We first describe a computational model for interpretation of human-object interaction videos. We identify three classes of human movements involved in interactions with manipulable objects. These movements are 1) reaching for an object of interest, 2) grasping the object, and 3) manipulating the object. These movements are ordered in time. The reach movement is followed by grasping which precedes manipulation. In our model, we ignore the grasping motion since the hand movements are too subtle to be perceived at the resolution of typical video cameras when the whole body and context are imaged.

3.1 Overview We present a graphical model for modeling human-object interactions. The nodes in the model correspond to the perceptual analyses corresponding to the recognition of objects, reach motions, manipulation motions, and object reactions. The edges in the graphical model represent the interactions/dependencies between different nodes. Reach movements enable object localization since there is a high probability of an object being present at the endpoint of a reach motion. Similarly, object recognition disables false positives in reach motion detection, since there should be an object present at the endpoint of a reach motion (see Fig. 3). Reach motions also help to identify the possible segments of video corresponding to manipulation of the object, since manipulation motion is preceded by reach motion. Manipulation movements provide contextual information about the type of object being acted on and object class provides contextual information on possible interactions with them, depending on affordances and function. Therefore, a joint estimation of the two perceptual elements provides better estimates as compared to the case when the two are estimated independently (see Fig. 4). The object reaction to a human action, such as pouring liquid from a carafe into a cup or pressing a button that activates a device, provides contextual information about

4

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. XX,

XX 2009

Fig. 3. Importance of contextual information involved in reach motions and object perception. (a) Object Detectors tend to miss some objects completely (original detector). (b) Lowering the detection threshold can lead to false positives in detection. The likelihood of a pixel being the center of the cup is shown by intensity of red (likelihood P ðOjeO Þ). (c) Reach Motion Segmentation also suffers from false positives (reach P ðMr jer Þ). The trajectories are shown in green and blue with possible endpoints of reach motion shown in red. (d) Joint probability distribution reduces the false positives in reach motion and false negatives in object detection (P ðO; Mr jeO ; er Þ).

Fig. 4. Importance of contextual information from interaction motion in object class resolution. In this experiment, object detectors for cups and spray were used. (a) The likelihood value of a pixel being the center of cup and spray bottle is shown by intensity of red and green, respectively (likelihood P ðOjeO Þ). (b) Hand trajectory for interaction motion (includes reach and manipulation). (c) The segmentation obtained. The green track shows the reach while the red track shows the manipulation. (d) Likelihood values after belief propagation (belief: BelðOÞ). By using context from interaction with the object, it was inferred that, since the object was subjected to a wave like motion, it is more likely a spray bottle.

the object class and the manipulation motion. Our approach combines all these types of evidences into a single video interpretation framework. In the next section, we present a probabilistic model for describing the relationship between different elements in human-object interactions.

3.2 Our Bayesian Model Our goal is to simultaneously estimate object type, location, movement segments corresponding to reach movements, manipulation movements, type of manipulation movement and their effects on objects by taking advantage of the contextual information provided by each element to the others. We do this using the graphical model shown in Fig. 5. In the graphical model, objects are denoted by O, reach motions by Mr , manipulation motions by Mm , and object reactions by Or . The video evidence is represented by e ¼ feO ; er ; em ; eor g, where eO represents object evidence, er and em represent reach and manipulation motion evidence, and eor represents object reaction evidence. Using Bayes rule and conditional independence relations, the joint probability distribution can be decomposed as3
P ðO; Mr ; Mm ; Or jeÞ / P ðOjeO ÞP ðMr jOÞP ðMr jer Þ . . . . . . P ðMm jMr ; OÞP ðMm jem ÞP ðOr jO; Mm ÞP ðOr jeor Þ: We use loopy belief propagation algorithm [40] for inference over the graphical model. In the next few sections we discuss how to compute each of these terms. Section 3.3
3. All of the variables are assumed to be uniformly distributed and, hence, P ðOÞ, P ðMr Þ, P ðMm Þ, P ðOr Þ, P ðeO Þ, P ðer Þ, P ðem Þ, and P ðeor Þ are constant. 4. We use linear gradient voting with nine orientation bins in 0-180; 12 Â 12 pixel blocks of four 6 Â 6 pixel cells.

Fig. 5. Underlying Graphical Model for Human-Object Interaction. The observed and hidden nodes are shown in gray and white, respectively.

discusses how to compute the object likelihoods P ðOjeO Þ. In Section 3.4.1 we explain the computation of reach motion likelihood, P ðMr jer Þ, and the contextual term P ðMr jOÞ. This is followed by a discussion on computation of manipulation motion likelihood, P ðMm jem Þ, and the term P ðMm jMr ; OÞ in Section 3.4.2. In Section 3.5, we discuss the object reaction likelihood P ðOr jeor Þ and the prior term, P ðOr jO; Mm Þ.

3.3 Object Perception The object node in the graphical model represents the random variable O. We want to estimate the likelihood of the type of object and the location of the object. While our approach is independent of the likelihood model, we employ a variant of the histogram of oriented gradient (HOG) approach from [9], [62].4 Our implementation uses a cascade of adaboost classifiers in which the weak classifiers are Fischer Linear Discriminants. This is a window-based detector; windows are rejected at each cascade level and a

GUPTA ET AL.: OBSERVING HUMAN-OBJECT INTERACTIONS: USING SPATIAL AND FUNCTIONAL COMPATIBILITY FOR RECOGNITION

5

Fig. 6. Results of upper body pose estimation algorithm.

window which passes all levels is classified as a possible object location. Based on the sum of votes from the weak classifiers, for each cascade level, i, we compute the probability Pi ðwÞ of a window, w, containing the object. If a window were evaluated at all cascade Q levels, the probability of it containing an object would be L Pi ðwÞ. However, for computai¼1 tional efficiency many windows are rejected at each stage of the cascade.5 The probability of such a window containing an object is computed based on the assumption that such windows would just exceed the detection threshold of the remaining stages of the cascade. Therefore, we also compute a threshold probability (P ti ) for each cascade level i. This is the probability of that window containing an object whose adaboost score was at the rejection threshold. If a detector consists of L levels, but only the first lw levels classify a window w as containing an object, then the overall likelihood is approximated by P ðO ¼ fobj; wgjeO Þ %
lw Y i¼1

Fig. 7. Plot on the left shows velocity profiles of some mass-spring motions and the figure on the right shows some ballistic hand movements. The velocity remains low and constant during mass-spring movements. It reduces to zero only at the end of the movement. On the other hand, hand movements corresponding to ballistic motion such as reach/strike have distinct “bell” shapes.

Pi ðwÞ

L Y j¼lw þ1

ðP tj Þ:

ð1Þ

3.4 Motion Analysis We need to estimate the likelihoods of reach motion and manipulation motion. Our likelihood model is based on hand trajectories and, therefore, requires estimation of endpoints (hands in case of upper body pose estimation) in each frame. While one can use independent models for tracking the two hands, this could lead to identity exchange and lost tracks during occlusions. Instead, we pose the problem as upper body pose estimation. We implemented a variant of [12] for estimating the 2D pose of the upper body. In our implementation, we use an edge [20] and silhouettebased likelihood representation for body parts. We also use detection results of hands based on shape and appearance features and a temporal tracking framework where smoothness constraints are employed to provide priors. Fig. 6 shows the results of the algorithm on few poses. 3.4.1 Reach Motion The reach motion is described by three parameters: the start time (tr ), the end time (tr ), and the 2D image location being s e reached for (lr ). We want to estimate the likelihood of reach motion (Mr ¼ ðtr ; tr ; lr Þ) given the hand trajectories. An s e approach for detecting reach motion was presented in [42]. It is based on psychological studies which indicate that the
5. Our experiments indicate that in many cases locations rejected by a classifier in the cascade are true object locations and selected by our framework.

hand movements corresponding to ballistic motion such as reach/strike have distinct “bell” shaped velocity profiles [31], [47] (see Fig. 7). There is an initial impulse accelerating the hand/foot toward the target, followed by a decelerating impulse to stop the movement. There is no mid-course correction. Using features such as time to accelerate, peak velocity, and magnitude of acceleration and deceleration, the likelihoods of reach movements can be computed from hand trajectories. However, there are many false positives because of errors in measuring hand trajectories. These false positives are removed using contextual information from object location. In the case of point mass objects, the distance between object location and the location being reached for, should be zero. For a rigid body, the distance from the center of the object depends on the grasp location. We represent P ðMr jOÞ using a normal function, N ðjlr lo j; ; Þ, where  and  are the average distance and variance of the distances in a training database between grasp locations and object centers.

3.4.2 Manipulation Motion Manipulation motions also involve three parameters: start time (tm ), end time (tm ), and the type of manipulation s e motion/action (Tm ) (such as answering a phone, drinking, etc.). We need to compute P ðMm jem Þ, the likelihood of a manipulation given the evidence from hand trajectories. While one can use any gesture recognition approaches based on hand trajectories to estimate the likelihood, we use a simple discrete HMM-based approach to estimate it. We need to first compute a discrete representation of the manipulation motion. Toward this end, we obtain a temporal segmentation of the trajectory based on a limb propulsion model. An approach for such a segmentation was presented in [42]. There are two models for limb propulsion in human movements: ballistic and mass-spring models [47]. Ballistic movements, discussed previously, involve impulsive propulsion of the limbs (acceleration toward the target followed by deceleration to stop the movement). In the massspring model, the limb is modeled as a mass connected to a spring. Therefore, the force is applied over a period of time. To obtain the temporal segmentation of a velocity profile, it is observed that the endpoints of each ballistic segment correspond to a local minima in the velocity profile. However, due to noise all local minimas are not the endpoints of atomic segments. Therefore, the segmentation

6

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. XX,

XX 2009

Fig. 8. Segmentation Procedure: The first graph shows the local minima of velocity profile. These local minima are classified into possible endpoints of each segment. This is followed by a maximum likelihood approach to obtain the segmentation of the velocity profile.

problem is treated as that of classifying the points of local minima as being segmentation boundaries or not. The classification is based on features such as accelerating impulse and its duration. Given confidence values for each time instant to be a starting, ending, or negligible movement, we compute the most likely segmentation of the velocity profile using Maximum Likelihood (see Fig. 8). Each segment is then replaced by a discrete alphabet defined as the cross product of type of propulsion (ballistic/ mass-spring) and the hand locations at the end of the motion segments, represented with respect to the face. By using alphabets for atomic segments, we transform a continuous observation into a discrete symbol sequence. This is used as input to obtain the likelihoods of different types of manipulation motion from their corresponding HMMs. In addition to computing the likelihood, we need to compute the term P ðMm jMr ; OÞ. Manipulation motion is defined as a three-tuple, Mm ¼ ðtm ; tm ; Tm Þ. The starting and s e ending times, tm and tm , depend on Mr but are independent s e of O. Similarly, the type of manipulation motion, Tm , depends on O but is independent of Mr .6 Hence, we decompose the prior term as  Á À ð2Þ P ðMm jMr ; OÞ ¼ P tm ; tm Mr P ðTm jOÞ: s e Assuming that grasping takes negligible time, the time difference between the ending time of a reach motion and the starting time of a manipulation motion should be zero. We model P ðtm ; tm jMr Þ as a normal distribution s e N ðtm À tr ; 0; t Þ, where t is the observed variance in the s e training data set. P ðTm ¼ mtypejO ¼ objÞ is computed based on the number of occurrences of manipulation mtype on object obj in our training data set.

Fig. 9. Using appearance histograms around hand to estimate P ðOr jeor Þ. In the case above, illumination change due to flashlight causes the change in intensity histogram.

motion is hard to detect, the effect of such manipulation (the lighting of the flashlight) is easy to detect. Similarly, the observation of object reaction can provide context on object properties. For example, the observation of the effect of pouring can help making the decision of whether a cup was empty or not. The parameters involved in object reaction are the time of reaction (treact ) and the type of reaction (Tor ). However, measuring object reaction type is difficult. Mann et al. [30] presented an approach for understanding observations of interacting objects using Newtonian mechanics. This approach can only be used to explain rigid body motions. Apart from rigid body interactions, the interactions which lead to changes in appearances using other forces such as electrical are also of interest to us. We use the differences of appearance histograms (eight bins each in RGB space) around the hand location as a simple representation for reaction type classification. Such a representation is useful in recognizing reactions in which the appearance of the object at the time of reaction, treact , would be different than appearance at the start or the end of the interaction. Therefore, the two appearance histograms are subtracted and compared with the difference histograms in the training database to infer the likelihood of the type of reaction (Tor ). In addition, we need to compute the priors P ðOr jMm ; OÞ. Object reaction is defined by a two-tuple, Or ¼ ðTor ; treact Þ. Using the independence of the two variables: P ðOr jMm ; OÞ ¼ P ðTor jMm ; OÞP ðtreact jMm ; OÞ: ð3Þ

3.5 Object Reactions Object reaction is defined as the effect of manipulation on the object. In many cases, manipulation movements might be too subtle to observe using computer vision approaches. For example, in the case of a flashlight, the manipulation involved is pressing a button. While the manipulation
6. Type of manipulation also depends upon the direction of reach motion. This factor is, however, ignored in this paper.

The first term can be computed by counting the occurrences of Tor when the manipulation motion is of type mtype and the object is of type obj. For modeling the second treact Àtm term, we observed that the reaction-time ratio, rr ¼ ðtm ÀtmsÞ , is e s generally constant for a combination of object and manipulation. Hence, we model the prior by a normal function N ðrr ; r ; r Þ over the reaction-time ratio, where r and r are the mean and variance of reaction-time ratios in the training data set.

GUPTA ET AL.: OBSERVING HUMAN-OBJECT INTERACTIONS: USING SPATIAL AND FUNCTIONAL COMPATIBILITY FOR RECOGNITION

7

Fig. 10. Examples depicting the reasoning process in action inference from static images. The labels in red are the result of a scene categorization process, cyan labels and blue labels represent scene and manipulable objects, respectively, and the magenta label is the result of a pose estimation algorithm. For understanding actions from static images, information is combined from all components. While the pose is similar in both scenes, the presence of the racket and tennis ball, along with the tennis court environment suggests that the first picture is a “tennis-forehand” while the second is baseball pitching due to the presence of the pitching area and the baseball field. (a) Tennisforehand. (b) Baseball pitching.

4

RECOGNIZING INTERACTIONS FROM STATIC IMAGES

While action recognition requires motion information, in the case of static images, contextual information can be used in conjunction with human pose to infer action. Figs. 10a and 10b show examples of reasoning involved in inference of actions from a static image. In both cases, pose alone does not provide sufficient information for identifying the action. However, when considered in the context of the scene and the objects being manipulated, the pose become informative of the goals and the action. Relevant objects in the scene generally bear both a semantic7 and spatial relationship with humans and their poses. For example, in a defensive stance of a cricket batsman, the bat is facing down and is generally below or in level with the person’s centroid. Similarly, the location of the cricket ball is also constrained by the person’s location and pose (see Fig. 11). We describe how to apply spatial constraints on locations of objects in the action recognition framework. By combining action recognition from poses with object detection and scene analysis, we also improve the performance of standard object detection algorithms. We first present an overview of the approach in Section 4.1. Section 4.2 describes our Bayesian model for recognition of actions and objects in static images. This is followed by a description of individual likelihood models and interactions between different perceptual elements in subsequent sections.

Fig. 11. Detection of manipulable objects can be improved using spatial constraints from human action. The ball detector detects two possible cricket balls. In the case of defensive batting, the probability of possible locations of the ball is shown by the shaded regions. Hence, the region below the centroid, where the ball is more likely to be present, is brighter. The ball denoted in box 4 lies in a darker region, indicating it is less likely to be a cricket ball due to its location with respect to the human. For objects such as bats, another important spatial constraint is connectedness. A segment of the bat should be connected to a segment of the human; therefore, false positives, such as object 1, can be rejected. (a) Without spatial constraints. (b) With spatial constraints.

4.1 Overview Studies on human-object perception suggest that people divide objects into two broad categories: scene and manipulable objects. These objects differ in the way inferences are made about them. Chao and Martin [8] showed that when humans see manipulable objects, there is cortical activity in the region that corresponds to action execution. Such responses are absent when scene objects, such as grass and house, are observed. Motivated by such studies, we treat the
7. By semantic relationships we refer to those relationships that are captured by co-occurrence statistics.

two classes differently in terms of the role they play in inferring human location and pose and represent them by two different types of nodes in the Bayesian model. Our Bayesian model consists of four types of nodes, corresponding to scene/event, scene objects, manipulable objects, and human. The scene node corresponds to the place where the action is being performed, such as a cricket ground or a tennis court. The scene object nodes correspond to objects which do not have causal dependency on the human actor and are mostly fixed in the scene, such as the net in the tennis court. Manipulable objects correspond to the instruments of the game such as a ball or a racket. The interactions between these nodes are based on semantic and spatial constraints. The type of objects that occur in an image depends on the scene in which the action takes place. For example, it is more likely for a pitch to occur in a cricket ground than a tennis court. Therefore, there exist semantic relationships between scene and scene objects. The type of action corresponding to a pose depends on the type of scene and the scene objects present. The type of action also depends on the location of the human with respect to the scene objects. For example, a pose with one hand up in a tennis court can either be a serve or a smash. However, if the human is located at the baseline it will more likely be a serve; otherwise, if he is near the net it will more likely be a smash. While considering that such spatial relationships are important, in this paper, we consider only the semantic relationships between actions and the scene and scene objects. Since we are not modeling spatial relationships between scene objects and human actions, we only consider the presence/absence of scene objects. Therefore, each scene object node (representing a class such as cricket-stumps) is characterized by a binary variable indicating the presence/absence of that scene object class.

8

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. XX,

XX 2009

Fig. 12. Graphical model. The observed and hidden nodes are shown in blue and white, respectively.

For manipulable objects, there exist both spatial and semantic constraints between people and the objects. The type of manipulable objects in the image depends on the type of action being performed. Also, the location of the manipulable objects is constrained by the location of the human, the type of action and the types of manipulable objects. For example, the location of a tennis ball is constrained by the type of action (in the case of a forehand the ball is located to the side of a person while in the case of a serve it appears above). Spatial constraints also depend on the type of object; objects such as a tennis racket should be connected to the person while objects such as a ball generally have no such connectivity relationships. We describe an approach to represent such relationships in our Bayesian network.

individual objects in their scene classification framework. Rather than looking at a scene as a configuration of objects, they propose to consider a scene like an individual object, with a unitary shape. They show that scenes belonging to the same category share a similar and stable spatial structure that can be extracted at once, without segmenting the image. A set of holistic spatial properties of the scene, together referred to as a Spatial Envelope, are used, which include naturalness, openness, roughness, ruggedness, and expansion. We use their approach to compute the concatenated feature vector for every image in the data set. Using the training feature vectors we train a Support Vector Machine (SVM) for the classification task. For a test image, the SVM returns a score dS which represents the distance of the test point from the separating hyperplane. Based on this distance, we estimate the probability P ðSjeS Þ as P ðSjeS Þ ¼ 1 expðÀScene dS Þ; ZScene ð5Þ

where Scene is the scaling parameter and ZScene is the normalization factor.

4.2 Our Bayesian Model The graphical model used for the scene interpretation framework is shown in Fig. 12. We simultaneously estimate the scene type, scene objects, human action, and manipulable object probabilities. Let S represent the scene variable, SO1 . . . SON represent the N type of scene objects, H represent the human, and MO1 ::MOM represent the M possible manipulable objects. If e ¼ feS ; eSO1 ::eSON ; eH ; eMO1 ::eMON g represents the evidential variables or the observations, our goal is to estimate P ðS; H; SO1 ::SON ; MO1 ::MOM jeÞ. This can be decomposed as Y P ðMOj jHÞP ðMOj jeMOj ÞP ðHjS; SO1 ::SON ÞP ðHjeH Þ . . .
j

4.4 Scene Objects Each scene object node corresponds to a class of scene objects and is represented by the probability of presence of that object class across the image. We uniformly sample points across the image and extract a patch around each point (for experiments, grid points are sampled at 25 pixels each in the x, y direction and the patch size of 50 Â 50 is used). We classify each patch as belonging to one of the N scene object classes, using an adaboost-based classifier [55] based on features such as HOG, histograms of each color channel (eight bins each in color channel), and histograms of edge distance map values within the neighborhood. We compute P ðSOi jSÞ based on the conditional probability tables learned using the co-occurrence relationships in the training data set. 4.5 Human in Action Every detected person in the image is characterized by the action (A) he is performing, and location given by a bounding box (lH ). For action classification, we detect humans and employ the pose information. A similar approach has been proposed in a recent paper [13]. In our experiments, we detect humans using an approach similar to [59]. Since the observed image shape of a human, changes significantly with articulation, viewpoint, and illumination, it is infeasible to train a single human detector for all shapes. Instead, we first cluster the observed shapes from our training data, and train multiple human detectors, one for each shape cluster. Our human detectors closely match those proposed by [9]. Given a bounding box around a detected human, we segment the human using GrabCut [4], an efficient tool for foreground segmentation. Once we have a possible human segmentation, we extract shape context features (5 radial bins and 12 orientation bins) from the silhouette of the human. We then cluster shape context features [1] from the training database to build a dictionary of “shape context words.” A detected human in an image is then characterized by the histogram of shape context words. The number of words/clusters determines the dimensionality of our pose feature vector. We then use the K-Nearest

...

Y
i

P ðSOi jSÞP ðSOi jeSOi ÞP ðSjeS Þ: ð4Þ

We use the loopy belief propagation algorithm [40] for inference over the graphical model.

4.3 Scene Perception A scene is mainly characterized as a place in which we can move [39]. In this paper, the scene corresponds to the place where an action is being performed such as tennis court and croquet field. Each image is associated with a probability of belonging to one of the scene classes. Several experimental studies have shown that when humans view a scene, they extract functional and categorical information from the scene, whereas they tend to ignore information regarding specific objects and their locations. In accordance, Oliva and Torralba [39] bypass the segmentation and processing of

GUPTA ET AL.: OBSERVING HUMAN-OBJECT INTERACTIONS: USING SPATIAL AND FUNCTIONAL COMPATIBILITY FOR RECOGNITION

9

Neighbor approach for classification, providing P ðHjeH Þ. Given a test sample, we determine the K-nearest neighbors in the training data. Each of the K neighbors vote for the class it belongs to with a weight based on its distance from the test sample. The final scores obtained for each class determine the likelihoods for each pose category, P ðHjeH Þ. For the experiments used in the paper, we use K ¼ 5. We also need to compute P ðHjS; SO1 ::SON Þ. Assuming conditional independence between scene object categories given human action, we rewrite as P ðHjS; SO1 ::SON Þ ¼
N Y i

P ðHjS; SOi Þ:

ð6Þ

Each of these can be computed using co-occurrence statistics of human action-scene-scene object combinations, independently for every scene object class.

Fig. 13. Spatial constraints between locations of manipulable objects and humans for different poses. In an idealized scenario, for a forehand pose, the ball is more likely to be seen on the side; for a tennis-serve, it is more likely to be seen above the human. We use two radial bins and eight orientation bins to specify position of manipulable object with respect to the human body.

4.6 Manipulable Objects Each detected manipulable object in the image has the following attributes: an associated class id (cm ) and location i parameters given by a bounding box (lm ) around the object. i We use the object detector described in Section 3.3. Using this approach, however, we are unable to distinguish between objects that have the same shape but a different dominant color; for example, a cricket ball (often red or white in color) as opposed to a tennis ball (often yellow in color). Thus, we build appearance models of manipulable objects using nonparametric Kernel Density Estimation (KDE) to also perform an appearance-based classification. We sample pixels from training images of the manipulable objects and build a 3D model in the RGB space.
pModel ðr; g; bÞ ¼
N 1X Kr ðr À ri ÞKg ðg À gi ÞKb ðb À bi Þ: ð7Þ N i¼1

likely to the side of the player. We model positional relations in terms of the displacement vector of the object centroid from the centroid of the human body. Thus, we obtain Á Á À  Á À Á À  À P MOi ¼ cm ; lm jH ¼ ðA; lH Þ ¼ P lm cm ; A; lH P cm A : i i i i i ð9Þ The first term refers to the spatial constraints and can be learned by discretizing the space around the human as shown in Fig. 13. From the training images, we learn the condition probability tables of the region in which the manipulable object lies given the type of manipulable object and the type of action. The second term is the semantic constraint and is modeled from co-occurrence statistics of human action-manipulable objects combinations from training data.

Given a test image, we first use the shape-based classifier to detect potential object candidates. Within each candidate window, we sample pixels and build a density estimate using KDE. This test density is compared to the color model of every object category using the Kullback-Leibler distance. This provides the final manipulable object detection probabilities based on appearance given by P ðMOi jeap i Þ. MO Therefore, the probability P ðMOi jeMOi Þ is given by   Á À Á À ð8Þ P ðMOi jeMOi Þ ¼ P MOi esh i P MOi eap i ; MO MO where esh refers to shape and eap refers to appearance evidence. We also need to compute P ðMOi jHÞ. Human actions and locations provide both semantic and spatial constraints on manipulable objects. The spatial constraints given human locations are with respect to the type of manipulable object and type of action being performed. We model two kinds of spatial constraints: 1) Connectivity— Certain manipulable objects like a tennis racket or a cricket bat should be connected to the human in action. 2) Positional and Directional Constraints—These location constraints are evaluated with respect to the centroid of the human that is acting on them. The conditional probability densities are based on the type of action being performed. For example, given a tennis-serve action, it is more likely that the ball is above the player, while, if the action is forehand, it is more

5

EXPERIMENTAL EVALUATION

5.1 Video Interpretation We evaluated our video interpretation framework on test data set8 of 10 subjects performing six possible interactions with four different objects. The objects in the test data set included cup, spray bottle, phone, and flashlight. The interactions with these objects were drinking from a cup, spraying from a spray bottle, answering a phone call, making a phone call, pouring from a cup, and lighting the flashlight. Training. We used a fully supervised approach for training the Bayesian model for video interpretation. Training of the model requires training of a HOG-based detector for all object classes and HMM models for all classes of interactions. Training for HOG-based object detector was done using images from training data sets obtained using Google image search (50 images for each object, negative images were used from INRIA and CALTECH data sets). HMM models were trained using a separate training data set of videos. The object reactions are learned using the supervised training scheme. In training videos, the frames for the object reaction were manually segmented and the appearance histograms around the hand were used to learn
8. The data sets used in all the experiments are available online and can be downloaded from http://www.umiacs.umd.edu/~agupta.

10

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. XX,

XX 2009

tion of reach and manipulation motion. The segmentation error was the difference between the actual frame number and the computed frame number for the end of a reach motion. We obtained the ground truth for the data using manual labelings. Fig. 17 shows the histogram of segmentation errors in the videos of the test data set. It can be seen that 90 percent of detections were within three frames of actual end-frames of reach motion. The average length of the video sequence was approximately 110 frames.
Fig. 14. Object likelihood confusion matrix: The ith row depicts the expected likelihood values when ith type of object is present. The right table shows the results of our whole framework, taking into account action, object reaction, and reach motion. (a) HOG detector. (b) Using whole framework.

the appearance of object reaction. Additionally, our model requires co-occurrence statistics of object-interaction-reaction combinations, distance between grasp location and object center, and reaction-time ratios. We used a training data set of 30 videos of five actors performing different types of manipulations on the objects. Training was done in a fully supervised manner. All of the videos were manually labeled with object locations, hand locations and the type of objects, manipulation, and object reactions. Object classification. Among the objects used, it is hard to discriminate the spray bottle, flashlight, and cup because all three are cylindrical (see Figs. 16a and 16b). Furthermore, the spray bottle detector also fired for the handset of the cordless phone (see Fig. 16d). Our approach was also able to detect and classify objects of interest even in cluttered scenes (see Fig. 16c). Figs. 14a and 14b show the likelihood confusion matrix for both the original object detector and the object detector in the human-object interaction framework. Using interaction context, the recognition rate of objects at the end of reach locations improved from 78.33 percent to 96.67 percent.9 Action recognition. Of the six activities, it is very hard to discriminate between pouring and lighting on the basis of hand trajectories (see Figs. 16a and 16b). While differentiating drinking from phone answering should be easy due to the differences in endpoint locations, there was still substantial confusion between the two due to errors in computation of hand trajectories. Fig. 15a shows the likelihoods of actions that were obtained for all the videos using hand-dynamics alone. Fig. 15b shows the confusion matrix when action recognition was conducted using our framework. The overall recognition rate increased from 76.67 percent to 93.34 percent when action was recognized using the contextual information from objects and object reactions. While the trajectories might be similar in many cases, the context from object provided cues to differentiate between confusing actions. Similarly, in the cases of lighting and pouring, contextual cues from object reaction helped in differentiating between those two actions. Segmentation errors. Apart from errors in classification, we also evaluated our framework with respect to segmenta9. The recognition rate depicts the correct classification of localized object into one of the five classes: background, cup, spray bottle, phone, and flashlight.

5.2 Image Interpretation Data set. We evaluated our approach on a data set which had six possible actions: “tennis-forehand,” “tennis-serve,” “volleyball-smash,” “cricket-defensive shot,” “cricket-bowling,” and “croquet-shot.” The images for the first five classes were downloaded from the internet and for the sixth class, we used a publicly available data set [29]. A few images from the data set are shown in Fig. 19. The classes were selected so that they had significant confusion due to scene and pose. For example, the poses during “volleyball-smash” and “tennis-serve” are quite similar and the scenes in “tennisforehand” and “tennis-serve” are exactly the same. Training. We used a fully supervised approach for training the Bayesian model for image interpretation. We have to learn the parameters for individual likelihood functions and parameters of the conditional probabilities which model the interactions between different perceptual analyses. To learn parameters of individual likelihood functions, we trained individual detectors separately using training images from Google image search (50 images each for every object and 30 silhouettes each for the pose likelihood). Learning parameters corresponding to conditional probabilities requires a separate training data set of images. Our training data set consisted of 180 images (30 from each class). Evaluation. We tested the performance of our algorithm on a data set of 120 test images (20 from each class). We compared the performance of our algorithm with the performance of models based on isolated components. Fig. 20 shows the confusion matrix obtained using the full model described in the paper. We also show some failure cases in the figure. Our approach gave some misclassifications when the scene involved is the same but actions are different such as bowling being classified as batting. This occurs whenever the pose classification algorithm gives a wrong action likelihood (mostly due to faulty segmentation by Grabcut) and the manipulable object detector fails to find any discriminating manipulable object. Fig. 21a shows the performance of a pose-based classification algorithm. We used the pose component of our model to obtain the confusion matrix. As expected, the performance of pose-only model is very low due to similar poses being shared by different actions. For example, there is high confusion between “tennis-serve” and “bowling,” since both actions share a high arm pose. Similarly, we see confusion between “bowling” and “volleyball.” The confusion between “volleyball-smash” and “tennis-forehand” is mainly due to incorrect segmentations by grabcut. The comparison between overall performance of our approach and the individual components is shown in Fig. 21b. The performance of our approach was 78.86 percent

GUPTA ET AL.: OBSERVING HUMAN-OBJECT INTERACTIONS: USING SPATIAL AND FUNCTIONAL COMPATIBILITY FOR RECOGNITION

11

Fig. 15. Comparison of action likelihoods without and with contextual information. Each column represents the normalized likelihood values for six possible actions. (a) HMM-based action recognition. (b) HMM-based recognition in interaction context.

as compared to 57.5 percent by the pose-only model and 65.83 percent by the scene-only model. Figs. 22 and 23 show some examples of correct classification by our algorithm. In both cases, our approach rejects false positives because the belief in the objects falls below the detection threshold when combined with other elements like pose and scene information. For example, in Fig. 22, the false positives of bats are rejected as they fail to satisfy spatial constraints. Also, in both cases, detections related to objects incongruent with scene and action information are also rejected. Influence of parameters. We evaluated our system with respect to the parameters of each component of our system. We varied the parameter Scene used to obtain the scene classification probabilities (Section 4.3). Fig. 24a shows that action recognition accuracy increases with increasing Scene , but flattens out after a value of 5. The discriminative power of the scene component lowers with decreasing Scene and, therefore, we observe a lower system performance. In our experiments, we use Scene ¼ 5. Oliva and Torralba [39] use the Windowed Discriminant Spectral Template (WDST) which describes how the spectral components at different spatial locations contribute to a spatial envelope property, and sample it at regular intervals to obtain a discrete representation. One of the components of their method, wScene , determines the coarseness of this sampling interval. We varied the coarseness of the sampling where smaller wScene refers to coarser sampling. Fig. 24b shows our performance accuracy with respect to wScene . Our action recognition accuracy reduces for a very coarse sampling of the WDST, but is stable at finer scales. We use wScene ¼ 4 for the experiments. Our object detection module detects multiple objects in the scene and passes the top few detections onto the Bayesian framework. We evaluated our system accuracy

Fig. 16. Results of object detection in the human-object interaction framework. The likelihoods of the centers of different objects are shown in different colors. The colors red, green, cyan, and magenta show the likelihoods of cup, spray bottle, flashlight, and phone, respectively. (a) A flashlight is often confused as spray bottle by the HOG detector. However, when context from the framework is used there is no confusion. (b) Similarly a cup is often confused with a wide spray bottle. (c) Our detector can find and classify objects in clutter. (d) A spray bottle detector often fires at the handset of cordless phones due to the presence of parallel lines. However, such confusion can be removed using our framework. Also note that the users do not have to wear longsleeved clothing for the hand tracking to work, since the pose estimation framework uses a likelihood model based on edges and background subtraction in addition to skin color for searching the correct pose.

Fig. 17. Segmentation error histogram.

12

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. XX,

XX 2009

Fig. 18. Object recognition using contextual cues from reach, manipulation, and object reaction. As before, the colors red, green, cyan, and magenta show the likelihoods of cup, spray bottle, flashlight, and phone, respectively. The activities in the four cases above are drinking, pouring, lighting, and spraying, respectively.

Fig. 19. Our data set.

Fig. 20. Confusion matrix (full model): The figure shows the confusion matrix obtained using the full model. We also show some failure cases in the adjoining boxes. (a) The scene in these cases is classified correctly as cricket ground; however, due to faulty segmentations, the hands of the bowler are missed and the pose is misclassified as batting. (b) The pose is again misclassified as that of forehand due to some extra regions added to human segment. The missed detection (shown in dotted blue) of croquet bat also contributes to the misclassification. (c) In both the cases the segmentation fails, leading to inclusion of net with the human segment. (d) Apart from the error in the pose module, the racket is also missed and the ball is not present in the scene.

GUPTA ET AL.: OBSERVING HUMAN-OBJECT INTERACTIONS: USING SPATIAL AND FUNCTIONAL COMPATIBILITY FOR RECOGNITION

13

Fig. 21. (a) Confusion matrix (pose only): The confusion matrix is that only pose information is used for action classification. (b) Comparative performance of our approach with individual components.

with regards to the number of manipulable object detections passed to the Bayesian framework. For lower number of detections, the Bayesian framework has lower performance due to missing true detections. For higher number of detections, the Bayesian framework has lower performance due to the confusion from false positives. This effect is more pronounced for lower Scene values where the scene component has lower discriminativeness (see Fig. 24c). Finally, we evaluated our system with respect to the dimensionality of the pose feature vector. This dimensionality is determined by the number of “shape context words” formed in the shape dictionary. Fig. 24d shows the accuracy of our system against the dimensionality of the pose feature vector. As expected, our performance reduces when using a very small number of words. In our experiments, we use a dictionary of 100 visual words resulting in a 100-dimensional pose feature vector.

Fig. 23. Some other examples: In the first case, the tennis racket was detected with a lower likelihood as compared to other objects. After combining information from scene and action, the belief in the tennis racket increases since the action and the scene are tennis-serve and tennis court, respectively. In the second case, our approach rejects false positives of objects such as a mallet and bat. These objects are rejected, as they are not congruent to a volleyball-court and a volleyball-smash action. The false positives in volleyballs are also rejected as they fail to satisfy spatial constraints. Same abbreviations as in Fig. 22.

Fig. 22. Some illustrative examples showing the performance of the system. (a) The likelihood of various objects using independent detectors. The colors of the rectangles represent the likelihood probability (red meaning higher probability and blue meaning lower probability). (b) The posterior probabilities after the framework was applied. (c) The final result of our approach. In the first example, the detector detects four possible mallets and three possible croquet balls. After applying the spatial constraints, all the false positives are rejected as they fail to satisfy spatial constraints (the other mallets are not connected to a human body and the other balls are above the detected human centroid). In the second example, the false positives of bats are rejected as they fail to satisfy spatial constraints. Also, in both cases, detections related to objects incongruent with scene and action information are also rejected. (Note the abbreviations T-Ball, C-Ball, V-Ball, and Cq-Ball refer to tennis, cricket, volley, and croquet balls, respectively.)

14

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,

VOL. 31,

NO. XX,

XX 2009

[2]

[3] [4]

[5]

[6]

[7]

[8]

[9] Fig. 24. Influence of parameters on the system performance. (a) Scene . (b) wScene . (c) Number of manipulable objects. (d) Dimensionality of pose features.

[10]

[11]

6

CONCLUSION
[12]

Recent studies related to human information processing have confirmed the role of object recognition in action understanding and vice versa. Furthermore, neuropsychological studies have also shown that not only videos but also static images of humans in action evoke cortical responses in the brain’s motor area, indicating that humans tend to perceive dynamic information from static images as well. Motivated by such studies, we present two Bayesian models for interpretation of human-object interactions from videos and static images, respectively. Our approach combines the processes of scene, object, action, and object reaction recognition. Our Bayesian model incorporates semantic/functional and spatial context for both object and action recognition. Therefore, by enforcing global coherence between different perceptual elements, we can improve the recognition performance of each element substantially.

[13]

[14]

[15] [16]

[17]

[18]

[19]

ACKNOWLEDGMENTS
The authors would like to acknowledge VACE for supporting the research. The authors would like to thank Vitaladevuni Shiv and Mohamed Hussein for providing the code for reach motion detection and object detection, respectively. The authors would also like to thank the authors of [29] for providing the image data set for croquetshot action in the paper, the subjects who helped in collection of human-object interaction data set (videos), and Swati Jarial for the help in formation of static image data set. A preliminary version of this paper appears in [18].

[20]

[21]

[22]

[23]

[24]

REFERENCES
[1] A. Agarwal and B. Triggs, “3d Human Pose from Silhouettes by Relevance Vector Regression,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2004.

[25]

P. Bach, G. Knoblich, T. Gunter, A. Friederici, and W. Prinz, “Action Comprehension: Deriving Spatial and Functional Relations,” J. Experimental Psychology Human Perception and Performance, vol. 31, no. 3, pp. 465-479, 2005. A. Berg and J. Malik, “Geometric Blur for Template Matching,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2001. A. Blake, C. Rother, M. Brown, P. Perez, and P. Torr, “Interactive Image Segmentation Using an Adaptive GMMRF Model,” Proc. European Conf. Computer Vision, 2004. A. Bobick and A. Wilson, “A State-Based Approach to the Representation and Recognition of Gesture,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 19, no. 12, pp. 1325-1337, Dec. 1997. ˜ A. Bosch, A. Zisserman, and X. Munoz, “Image Classification Using Random Forests and Ferns,” Proc. IEEE Int’l Conf. Computer Vision, 2007. D. Bub and M. Masson, “Gestural Knowledge Evoked by Objects As Part of Conceptual Representations,” Aphasiology, vol. 20, pp. 1112-1124, 2006. L.L. Chao and A. Martin, “Representation of Manipulable ManMade Objects in Dorsal Stream,” NeuroImage, vol. 12, pp. 478-484, 2000. N. Dalal and B. Triggs, “Histogram of Oriented Gradients for Fast Human Detection,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2005. J. Davis, H. Gao, and V. Kannappan, “A Three-Mode Expressive Feature Model of Action Effort,” Proc. IEEE Workshop Motion and Video Computing, 2002. Z. Duric, J. Fayman, and E. Rivlin, “Function from Motion,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 18, no. 6, pp. 579-591, June 1996. P. Felzenszwalb and D. Huttenlocher, “Pictorial Structures for Object Recognition,” Int’l J. Computer Vision, vol. 61, pp. 55-79, 2005. V. Ferrari, M. Marin, and A. Zisserman, “Progressive Search Space Reduction for Human Pose Estimation,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2008. R. Filipovych and E. Ribeiro, “Recognizing Primitive Interactions by Exploring Actor-Object States,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2008. V. Gallese, L. Fadiga, L. Fogassi, and G. Rizzolatti, “Action Recognition in Premotor Cortex,” Brain, vol. 2, pp. 593-609, 1996. G. Guerra and Y. Aloimonos, “Discovering a Language for Human Activity,” Proc. Assoc. Advancement of Artificial Intelligence Workshop Anticipation in Cognitive Systems, 2005. A. Gupta, T. Chen, F. Chen, D. Kimber, and L. Davis, “Context and Observation Driven Latent Variable Model for Human Pose Estimation,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2008. A. Gupta and L. Davis, “Objects in Action: An Approach for Combining Action Understanding and Object Perception,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2007. A. Gupta and L. Davis, “Beyond Nouns: Exploiting Prepositions and Comparative Adjectives for Learning Visual Classifiers,” Proc. European Conf. Computer Vision, 2008. A. Gupta, A. Mittal, and L. Davis, “Constraint Integration for Efficient Multiview Pose Estimation with Self-Occlusions,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 30, no. 3, pp. 493-506, Mar. 2008. A. Gupta, J. Shi, and L. Davis, “A ’Shape Aware’ Model for SemiSupervised Learning of Objects and Its Context,” Proc. Conf. Neural Information Processing Systems, 2008. A. Gupta, P. Srinivasan, J. Shi, and L. Davis, “Understanding Videos, Constructing Plots—Learning a Visually Grounded Storyline Model from Annotated Videos,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2009. H.B. Helbig, M. Graf, and M. Kiefer, “The Role of Action Representation in Visual Object,” Experimental Brain Research, vol. 174, pp. 221-228, 2006. D. Hoiem, A. Efros, and M. Hebert, “Putting Objects in Perspective,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2006. S.H. Johnson-Frey, F.R. Maloof, R. Newman-Norlund, C. Farrer, S. Inati, and S.T. Grafton, “Actions or Hand-Object Interactions? Human Inferior Frontal Cortex and Action Observation,” Neuron, vol. 39, pp. 1053-1058, 2003.

GUPTA ET AL.: OBSERVING HUMAN-OBJECT INTERACTIONS: USING SPATIAL AND FUNCTIONAL COMPATIBILITY FOR RECOGNITION

15

[26] Z. Kourtzi, “But Still it Moves,” Trends in Cognitive Science, vol. 8, pp. 47-49, 2004. [27] Z. Kourtzi and N. Kanwisher, “Activation in Human MT/MST by Static Images with Implied Motion,” J. Cognitive Neuroscience, vol. 12, pp. 48-55, 2000. [28] Y. Kuniyoshi and M. Shimozaki, “A Self-Organizing Neural Model for Context Based Action Recognition,” Proc. IEEE Eng. Medicine and Biology Soc. Conf. Neural Eng., 2003. [29] L.-J. Li and L. Fei-Fei, “What, Where and Who? Classifying Events by Scene and Object Recognition,” Proc. IEEE Int’l Conf. Computer Vision, 2007. [30] R. Mann, A. Jepson, and J. Siskind, “The Computational Perception of Scene Dynamics,” Computational Vision and Image Understanding, vol. 65, no. 2, pp. 113-128, 1997. [31] R. Marteniuk, C. MacKenzie, M. Jeannerod, S. Athenes, and C. Dugas, “Constraints on Human Arm Movement Trajectories,” Canadian J. Psychology, vol. 41, pp. 365-378, 1987. [32] T.B. Moeslund, A. Hilton, and V. Kruger, “A Survey of Advances in Vision-Based Human Motion Capture and Analysis,” Computer Vision and Image Understanding, vol. 2, pp. 90-126, 2006. [33] D. Moore, I. Essa, and M. Hayes, “Exploiting Human Action and Object Context for Recognition Tasks,” Proc. IEEE Int’l Conf. Computer Vision, 1999. [34] H. Murase and S. Nayar, “Learning Object Models from Appearance,” Proc. Nat’l Conf. Artificial Intelligence, 1993. [35] K. Murphy, A. Torralba, and W. Freeman, “Graphical Model for Scenes and Objects,” Proc. Conf. Neural Information Processing Systems, 2003. [36] K. Murphy, A. Torralba, and W. Freeman, “Using the Forest to See the Trees: A Graphical Model Relating Features, Objects and Scenes,” Proc. Conf. Neural Information Processing Systems, 2004. [37] H. Nagel, “From Image Sequences towards Conceptual Descriptions,” Image and Vision Computing, vol. 6, no. 2, pp. 59-74, 1988. [38] K. Nelissen, G. Luppino, W. Vanduffel, G. Rizzolatti, and G. Orban, “Observing Others: Multiple Action Representation in Frontal Lobe,” Science, vol. 310, pp. 332-336, 2005. [39] A. Oliva and A. Torralba, “Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope,” Int’l J. Computer Vision, vol. 42, pp. 145-175, 2001. [40] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Network and Plausible Inference. Morgan Kaufmann, 1988. [41] P. Peursum, G. West, and S. Venkatesh, “Combining Image Regions and Human Activity for Indirect Object Recognition in Indoor Wide Angle Views,” Proc. IEEE Int’l Conf. Computer Vision, 2005. [42] V. Prasad, V. Kellokompu, and L. Davis, “Ballistic Hand Movements,” Proc. Conf. Articulated Motion and Deformable Objects, 2006. [43] A. Rabinovich, A. Vedaldi, C. Galleguillos, E. Wiewiora, and S. Belongie, “Objects in Context,” Proc. IEEE Int’l Conf. Computer Vision, 2007. [44] C. Rao, A. Yilmaz, and M. Shah, “View-Invariant Representation and Recognition of Actions,” Int’l J. Computer Vision, vol. 2, pp. 203-226, 2002. [45] E. Rivlin, S. Dickinson, and A. Rosenfeld, “Recognition by Functional Parts,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1994. [46] M. Shah and R. Jain, Motion-Based Recognition. Kluwer Academic, 1997. [47] I. Smyth and M. Wing, The Psychology of Human Movement. Academic Press, 1984. [48] L. Stark and K. Bowyer, “Generic Recognition through Qualitative Reasoning about 3D Shape and Object Function,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 1991. [49] E. Sudderth, A. Torralba, W. Freeman, and A. Wilsky, “Learning Hierarchical Models of Scenes, Objects and Parts,” Proc. IEEE Int’l Conf. Computer Vision, 2005. [50] J. Sullivan and S. Carlsson, “Recognizing and Tracking Human Action,” Proc. European Conf. Computer Vision, 2002. [51] S. Todorovic and N. Ahuja, “Learning Subcategory Relevances for Category Recognition,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2008. [52] A. Torralba and P. Sinha, “Statistical Context Priming for Object Detection,” Proc. IEEE Int’l Conf. Computer Vision, 2001. [53] C. Urgesi, V. Moro, M. Candidi, and S. Aglioti, “Mapping Implied Body Actions in the Human Motor System,” J. Neuroscience, vol. 26, pp. 7942-7949, 2006.

[54] L. Vaina and M. Jaulent, “Object Structure and Action Requirements: A Compatibility Model for Functional Recognition,” Int’l J. Intelligent Systems, vol. 6, pp. 313-336, 1991. [55] A. Vezhnevets and V. Vezhnevets, “‘Modest Adaboost’—Teaching Adaboost to Generalize Better,” Proc. Graphicon, 2005. [56] Y. Wang, H. Jiang, M. Drew, Z. Li, and G. Mori, “Unsupervised Discovery of Action Classes,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2006. [57] A. Wilson and A. Bobick, “Parametric Hidden Markov Models for Gesture Recognition,” IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 21, no. 9, pp. 884-900, Sept. 1999. [58] B. Wu and R. Nevatia, “Detection of Multiple, Partially Occluded Humans in a Single Image by Bayesian Combination of Edgelet Part Detectors,” Proc. IEEE Int’l Conf. Computer Vision, 2005. [59] B. Wu and R. Nevatia, “Detection and Tracking of Multiple Humans with Extensive Pose Articulation,” Proc. IEEE Int’l Conf. Computer Vision, 2007. [60] J. Wu, A. Osuntogun, T. Choudhury, M. Philipose, and J. Rehg, “A Scalable Approach to Activity Recognition Based on Object Use,” Proc. IEEE Int’l Conf. Computer Vision, 2007. [61] A. Yilmaz and M. Shah, “Actions Sketch: A Novel Action Representation,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2005. [62] Q. Zhu, S. Avidan, M. Ye, and K. Cheng, “Fast Human Detection Using a Cascade of Histograms of Oriented Gradients,” Proc. IEEE Conf. Computer Vision and Pattern Recognition, 2006. Abhinav Gupta received the MS degree in computer science from the University of Maryland in 2007 and the BTech degree in computer science and engineering from the Indian Institute of Technology, Kanpur, in 2004. He is a doctoral candidate in the Department of Computer Science at the University of Maryland, College Park. His research focuses on visually grounded semantic models and how language and vision can be exploited to learn such models. His other research interests include combining multiple cues, probabilistic graphical models, human body tracking, and camera networks. He has published numerous papers in prestigious journals and conferences on these topics. He has received several awards during his academic career including the University of Maryland Dean’s Fellowship for excellence in research. He is a member of the IEEE. Aniruddha Kembhavi received the bachelor’s degree in electronics and telecommunications engineering from the Government College of Engineering, Pune, India, in 2004. He is currently working toward the PhD degree in the Computer Vision Laboratory at the University of Maryland, College Park. His current research focuses on the problem of variable and feature selection for object classification. His research interests include human detection, object classification, and machine learning. He is a member of the IEEE. Larry S. Davis received the BA degree from Colgate University in 1970 and the MS and PhD degrees in computer science from the University of Maryland in 1974 and 1976, respectively. From 1977 to 1981, he was an assistant professor in the Department of Computer Science at the University of Texas, Austin. He returned to the University of Maryland as an associate professor in 1981. From 1985 to 1994, he was the director of the University of Maryland Institute for Advanced Computer Studies, where he is currently a professor, and is also a professor and the chair of the Computer Science Department. He was named a fellow of the IEEE in 1997.

. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.

Resource Allocation for Tracking Multiple Targets Using Particle Filters
Aniruddha Kembhavi† William Robson Schwartz Larry S. Davis † Department of Electrical Engineering Department of Computer Science University of Maryland, College Park, MD, USA
anikem@umd.edu, schwartz@cs.umd.edu, lsd@cs.umd.edu

Abstract
Particle ﬁlters have been very widely used to track targets in video sequences. However, they suffer from an exponential rise in the number of particles needed to jointly track multiple targets. On the other hand, using multiple independent ﬁlters to track in crowded scenes often leads to erroneous results. We present a new particle ﬁltering framework which uses an intelligent resource allocation scheme allowing us to track a large number of targets using a small set of particles. First, targets with overlapping posterior distributions and similar appearance models are clustered into interaction groups and tracked jointly, but independent of other targets in the scene. Second, different number of particles are allocated to different groups based on the following observations. Groups with higher associations (quantifying spatial proximity and pairwise appearance similarity) are given more particles. Groups with larger number of targets are given a larger number of particles. Finally, groups with ineffective proposal distributions are assigned more particles. Our experiments demonstrate the effectiveness of this framework over the commonly used joint particle ﬁlter with Markov Chain Monte Carlo (MCMC) sampling.

1 Introduction
The problem of object tracking has been the subject of a very large body of research. The vast improvement in the performance and the reduction in camera costs, coupled with ever increasing computing resources has led to the development of many sophisticated single and multitarget tracking algorithms. Speciﬁcally, the advent of intelligent surveillance systems has focused research efforts on the problem of detecting and tracking multiple humans. More recently, target tracking has been dominated by sequential Monte Carlo methods. The most popular Monte Carlo tracking method is the Condensation algorithm [8], commonly referred to as a particle ﬁlter. Particle ﬁlters have been used both widely and successfully to track objects in video sequences. However, as the number of tar-

gets to be jointly tracked increases, the dimensionality of the state space increases and the number of particles needed to sample this space rises exponentially. For a large number of targets often seen in surveillance videos, particle ﬁlters require an infeasible number of computations. Using multiple independent particle ﬁlters alleviates this tractability problem at the cost of performance. Independent particle ﬁlters often suffer from the problem of hijacking [10]. When two or more targets come close to one another, the target with the best likelihood score often hijacks the other ﬁlters, leading to tracking errors. Hijacking is boosted under the following conditions. Targets that lie in close proximity to one another in the state space may hijack the other’s ﬁlter. The proximity of two targets in the state space can be estimated by a measure of the overlap of their posterior probability distribution functions. Hijacking also increases when the interacting targets have similar appearance models. We present a particle ﬁltering framework along with an overlying graph structure that allows us to deal with the problem of hijacking without increasing the computational cost exponentially. A graph, whose nodes are the targets in the image and whose edges are a measure of the proximity and appearance similarity between targets is used to cluster the targets into multiple and possibly overlapping interaction groups. Targets that might hijack each other’s ﬁlters are grouped into a single group and tracked jointly, while targets in different groups are tracked independently. Our framework allows us to track all targets in the scene, while seamlessly allowing interaction groups to be formed and split at any time instant. The overall tracking framework is discussed in Section 3. An example of targets being clustered into different groups at different time instants is shown in Figure 1. We also address the problem of allocating resources to the different interaction groups identiﬁed in the scene. In the particle ﬁltering framework, the resources are essentially the total number of particles, which directly determines the computational cost. Given an upper bound on the number of particles to be used at any time instant, how

Figure 1. 3 frames from surveillance camera 1 monitoring a parking lot. The bounding rectangles show targets that are grouped together into a single interaction group and tracked jointly. Red rectangles show targets that are grouped individually, whereas the green and blue rectangles show groups of two and three targets respectively. Notice that the groups may overlap.

should we distribute them to minimize the number of tracking errors? Our solution to this optimization problem is based on the following observations. First, we assign more particles to groups with a larger number of targets due to their increased state space dimensionality. Second, we assign more particles to groups with higher associations. The association of a group quantiﬁes the spatial proximity and pairwise appearance similarities of the targets in the group. Third, we assign more particles to groups whose ﬁlters have a higher effective particle sample size at the previous time instant. This gives a measure of the ineffectiveness of the particle proposal distributions for each group. We further elaborate on the issue of resource allocation in Section 4. We demonstrate our particle ﬁltering framework on video sequences captured from two surveillance cameras overlooking a pedestrian walkway and a parking lot. Sequences from Camera 1 have up to 5 people simultaneously, while those from Camera 2 have up to 9 people simultaneously in the scene with an average height of just 27 pixels in the image. We provide comparisons to two other methods ﬁrst, using multiple independent particle ﬁlters and second, using a single joint particle ﬁlter with MCMC as a speed-up mechanism. Our results clearly demonstrate the superiority of our particle ﬁltering framework over the others, while using the same number of particles.

2 Related Work
Given the extensive amount of work carried out in the ﬁeld of object tracking, we are unable to provide an exhaustive literature review. Here, we present some of the more relevant bodies of work. For a good survey, we refer the reader to [16]. Many Bayesian approaches to the tracking problem have been explored before. When the posterior density of the targets at every time step can be assumed to be Gaussian, Kalman ﬁlters provide the optimal solution [3]. When such

constraints are invalid and need to be relaxed, the optimal solution typically becomes intractable. This has led to several approximation algorithms such as the extended Kalman ﬁlter, approximate grid-based ﬁlter [1] and the particle ﬁlter. The particle ﬁlter, originally introduced as the Condensation algorithm in the computer vision community, was proposed in [8], and has been widely and very successfully used for tracking targets [7][9][12]. There has also been research aimed at speeding up particle ﬁlters, so as to be able to track multiple targets in video sequences for real-time applications. Khan et al. replace the traditional importance sampling step in the particle ﬁltering framework by a Markov Chain Monte Carlo (MCMC) sampling step. This leads to an efﬁcient sampling of the target posterior distribution [10]. They also incorporate a Markov Random Field (MRF) to model interactions between targets to prevent hijacking. Zhao et al. [18] use MCMC with jump/diffusion dynamics to sample the posterior distribution. They also perform a detailed target occlusion analysis and are able to track many humans together in a crowded environment. In Section 5, we compare our particle ﬁltering framework with a single joint particle ﬁlter using MCMC as a speed-up mechanism. Our method obtains superior results when using the same number of particles, due to the appropriate particle distribution scheme. Yang et al. use a Hierarchical Particle Filter [15], whereby they break down the multi-feature observation likelihood to be computed in a coarse to ﬁne manner. This allows the computation to quickly focus on more promising regions and speeds up the entire system. They also employ optimized computational techniques such as Integral Histograms [13]. Khan et al. [11] introduce an efﬁcient method for using subspace representations in a particle ﬁlter. They apply Rao-Blackwellization to integrate out the subspace coefﬁcients in the state vector. Since part of the posterior is analytically calculated, the number of particles required decreases, which in turn speeds up the system. Brandao et

al. [2] speed up the particle ﬁltering approach by dividing the search space into subspaces that can be estimated separately. Low correlated subspaces are estimated with parallel or serial ﬁlters and their probability distributions are combined by a special aggregator ﬁlter. Sankaranarayanan et al. [14] present a method for implementing the particle ﬁlter using the Independent Metropolis Hastings sampler, that is highly amenable to pipelined implementations and parallelization. Zhou et al. [19] use an adaptive number of particles determined by the variance of the adaptive noise model at the current time step. As the variance decreases, the computational cost of the ﬁlter reduces. Gupta et al. [5] use an approach for camera selection and inference ordering to reduce the computational cost required to track people in a multi-camera framework. Yu et al. [17] present a decentralized approach to multiple tracking for the emerging application of sensor networks. In order to distribute the computation amongst multiple sensing units, they employ a set of autonomous and collaborative trackers. Dowdall et al. [4] also employ a distributed network of individual trackers. The interactions of these trackers are modeled using coalitional game theory. As in our tracking framework, such decentralized approaches address the tracker coalescence problem (the problem of hijacking ﬁlters). Our framework further allows us to allocate resources intelligently. This leads to superior tracking performance using a lesser number of particles.

3 Tracking Framework
Let Xt denote the state of the system and Zt denote the observation at time t. In a Bayesian tracking framework, we wish to estimate the posterior distribution P (Xt |Zt ) given in Equation (2). In a particle ﬁlter, the posterior distribution at time t is approximated by a set of particles with weights π r , as shown in Equation (3). P (Zt |Xt ) denotes the likelihood function and P (Xt |Xt−1 ) denotes the proposal distribution. In a regular particle ﬁlter, at every time step, new particles are generated from the particles at the previous time step using the proposal distribution, and their weights are obtained using the likelihood function. This gives the posterior distribution for the current time step. The number of particles needed to sample a higher dimensional state space increases exponentially with the dimensionality of the space (determined by the number of targets in the scene). P (Xt |Zt ) = P (Zt |Xt )P (Xt ) P (Zt ) P (Xt |Xt−1 )
Xt−1

Φ denotes the normalization constant. Let Nt be the total number of targets in the scene at time t. In our particle ﬁltering framework, at every time t, we cluster targets into possibly overlapping interaction groups, so that targets within a single group are tracked jointly, but targets in different groups are tracked independently of each other. This group structure can change at every time instant. The groups at j time t are denoted as gt ∀ j = {1, 2, .., Gt }, where Gt is the total number of groups at the time instant t. When referring to a single group, we will often drop the superscript j for ease of reading. We use λg to denote the set of all t n targets in group gt , and kt to be the set of all groups of which target n is a member at time t. As an example consider Figure 2(a). Based on the proximity and appearance of the 4 targets in the scene, 3 groups have been formed. 1 Thus, λ1 = {1},λ2 = {2, 3} and λ3 = {2, 4}. kt = {1}, t t t 2 3 4 kt = {2, 3}, kt = {2} and kt = {3}. The overall framework is summarized in Algorithm 1 and illustrated with an example in Figure 2. At time t − 1, Nt−1 targets are clustered into Gt−1 interaction groups. Each group is characterized by a joint distribution given by λg j P (Xt−1 |Z1:t−1 ). For each group gt−1 , new particles are proposed and their joint likelihoods are calculated (this is done independent of other groups in the scene). The resulting posterior distribution for each group is then marginalized to obtain the marginal posterior distribution for every target in the group. Targets belonging to multiple groups will thus have multiple such distributions. Inspired by work in the ﬁeld of sensor fusion based on particle ﬁlters [6], the particle ﬁlter corresponding to each group in the scene can be considered a logical sensor. For any given target, multiple distributions can be thought of as being generated by multiple logical sensors. Similar to [6], they can be combined linearly using appropriate mixture weights κg , to t−1 obtain a resultant posterior distribution per target. Thus, for every target n in the scene,
n P (Xt |Z1:t−1 ) =
n j∈kt−1

n (κj Pj (Xt |Z1:t−1 )) t−1

(4)

(1)

= ΦP (Zt |Xt ) P (Xt−1 |Zt−1 ) ≈ ΦP (Zt |Xt )
r

(2)
r r πt−1 P (Xt |Xt−1 )

(3)

Every group has a mixture weight which represents a measure of the group tracking conﬁdence. This conﬁdence value for each group is obtained at the previous time instant, and is determined by the likelihood estimates of the particles representing the corresponding group posterior distribution at time t − 1. Combining information from multiple ﬁlters has the added advantage of yielding more robust resultant distributions for each target in the scene. An example can be seen in Figure 2(b,c), where ﬁlter 2 has erroneous particles with large weights. However, since the mixture weight for ﬁlter 3 is higher than that of ﬁlter 2 (obtained from the previous time instant), the resultant posterior distribution for target 2 is improved. Using the resultant posterior distribution for each target

Zoomed View

Zoomed View

1
g1 g2

n2
g3

n2

2 3

4

n4 n3
(a) (b)

n4 n3
(c)

0.5

1
0.25 0.2 0.6 0.6

0.6 0.85 0.6 0.3 0.3

2 1
0.81

1 2 4 3 3
(e) (f)

2 4

3

0.73

4
(d)

Figure 2. A synthetic example demonstrating our tracking framework. (a) 4 targets n1 to n4 clustered into groups g1 to g3 at time t − 1, where n4 is moving south. (b) (Zoomed in view at time t). Particles representing the marginal distributions of the targets (blue for n2 , red for n3 , black for n4 ). n2 has twice the number of particles, representing two marginal distributions. Filter for g2 has erroneous particles with high weights since n2 and n3 have very similar appearances. However, ﬁlter for g2 has a lower mixture weight than the ﬁlter for g3 based on the likelihood estimates of the previous frame (not shown). (c) The resultant marginal distributions are thus more robust and have fewer erroneous particles with high weights. (d) Proximity (solid) and appearance similarity graphs (dashed). (e) New group structure at time t. (f) Maximum likelihood particles shown for all 4 targets in the scene.

in the scene, we build a proximity graph for all targets in the scene, where each node in the graph is a target and an edge between two nodes represents the similarity between the posterior distributions of the two targets. We ﬁrst estimate these non-parametric distributions from the corresponding particles using Kernel Density Estimation (KDE), and use the Kullback-Liebler (KL) distance between the two. Our state space for each target is 4-dimensional (x-position(x), y-position(y), width(w), height(h)). Using gaussian kernels K and M particles, the density estimate is given by, M 1 p(x, y, w, h) = ˆ Kσ (x − xm )Kσy (y − ym ) M m=1 x Kσw (w − wm )Kσh (h − hm ) (5) An appearance similarity graph is also built with each edge weight set equal to the similarity between the appearance models of the two targets. We model the appearance of targets using histograms in each color channel, and use the L2 distance as a distance measure. The proximity graph (PG) and appearance similarity graph (AG) are combined linearly to form a single similarity graph (SG) which is used to obtain a new group structure for the current time step t. SG(i, j) = αP G P G(i, j) + αAG AG(i, j)...∀i, j (6)

The issue of clustering targets into interaction groups based on the similarity graph is dealt with in the following section. Given the marginal distributions for every target and j the new group structure gt , j = {1, 2, .., Gt }, we need to obtain the appropriate joint distributions for each group. j For every group gt , we sample the marginal distributions of the interacting targets to obtain smaller subsets of particles, and combine them combinatorially to obtain particles for the joint distributions. For every new particle however, the joint likelihoods must be recalculated. Resampling is again performed on these particle distributions. The number of particles given by the resampling algorithm is set by the resource allocation function described in the following section. The number of particles assigned to each group at time t determines the computational cost incurred at the next time instant t + 1.

4 Resource Allocation
At every time step t, we use the Similarity Graph (SG) described in Equation (6) to distribute our resources so as to reduce the number of tracking errors. In a particle ﬁltering framework, our resources are the total number of particles

Algorithm 1 Overall tracking framework 1: for t = 2 to T do 2: Nt−1 targets in the scene are divided into Gt−1 groups, each represented by a joint density. j 3: for every gt−1 do 4: Propose particles and calculate their joint likelihoods. 5: Obtain marginal distributions for each target in the group (may lead to multiple marginal distributions for each target in the scene). 6: Resample each marginal distribution. 7: end for 8: for n = 1 to Nt−1 do 9: Combine marginal distributions for each target to obtain a resultant distribution per target. 10: end for 11: Build a proximity and an appearance graph. j 12: Obtain new groups gt using the greedy algorithm (Section 4). j 13: for every gt do 14: Combine marginals to form a new joint density. 15: Recalculate joint likelihoods for all particles representing the new joint distribution. 16: Resample to obtain appropriate number of particles given by the resource allocation function (Section 4). 17: end for 18: Given the joint likelihoods for every group, update group mixture weights. 19: end for

that can be used at every time step. Distribution of these particles at every time step is carried out in two stages. Targets that have a high similarity score with each other (nodes with large edge weights between them in the SG) must be grouped together (tracked jointly). Hence, in the ﬁrst stage, we cluster targets in the scene into interaction groups based on the SG. We deﬁne a binary decision variable φ(i, j) that determines if targets i and j will be grouped together, and a cost function C(g) that determines the cost of tracking targets in group g jointly. The optimization problem can be stated as follows,
Gt

max
∀i,j

φ(i, j)SG(i, j) such that min
i=1

i C(gt ) (7)

new edge. Thus the computation required at each iteration is kept low. The cost function C(g) is set to be quadratic in the number of targets in group g. In this stage, all groups having the same number of targets are assigned an equal cost, irrespective of the corresponding edges in the similarity graph. Thus the ﬁrst stage outputs the group structure for the current time step. In the second stage of resource allocation, we distribute particles amongst groups based on three criteria. The ﬁrst criterion is the strength of the Similarity Graph edges between targets in the group, given by the association of the group. We deﬁne the association of group gt as the average edge strength of all edges in the group, Assoc(gt ) = 1
len(gt ) 2 ∀i,j∈gt

We solve the above optimization problem using an approximate iterative greedy algorithm for the purpose of efﬁciency. First we set φ(i, j) = 0, ∀i, j. At every iteration of the algorithm, we select an edge (i , j ) with the maximum edge strength, set φ(i , j ) = 1 and recalculate the structure of the groups formed by the result of the addition of the edge (i , j ). If the cost of this group structure is less than the predetermined maximum cost, we go to the next iteration. If the cost of the group is above the maximum, we reset φ(i , j ) to 0 and the algorithm terminates. At every such iteration, given a graph with edges φ(i, j), the structure of the interaction groups is given by the set of all maximal cliques in the graph. Since we add only a single edge to the graph at every iteration, we compute an approximation to the set of all maximal cliques by updating the set of maximal cliques in the previous iteration with the

SG(i, j)

(8)

where len(gt ) is the number of targets in group gt . This encapsulates the proximity between the targets in the state space as well as the similarity of the appearances of all targets in the group. The number of particles assigned to the group increases with the association. The second criterion to allocate resources to a group is motivated by the degeneracy phenomenon seen in particle ﬁlters [1]. A common problem with particle ﬁlters is that with every iteration, the number of particles with nonnegligible weights decreases. The rate of degeneracy depends on the effectiveness of the particle proposal distributions (which involves the motion models learnt from the previous frames and variance estimates of the noise models). Though the degeneracy problem is overcome by the

use of resampling, we argue that a measure of group degeneracy, calculated before the particle resampling stage, should be used to allocate a larger or smaller number of particles to that group. This is because greater the effect of degeneracy, poorer the proposal distribution, larger the number of particles that must be assigned to ensure an effective search of the state space and reduce tracking errors. A suitable measure of degeneracy of the group particle ﬁlter is given by the Effective Sample Size (Pef f ) of the particles characterizing the posterior distribution of the group at the previous time instant [1]. This can be obtained as,
Ps −1 2 r πt−1

Pef f =
r=1

(9)

r where πt−1 represents the weight of the r’th particle at time instant t − 1, and Ps is the total number of particles. We deﬁne the fraction of effective particles (Fef f ) as Fef f = Pef f /Ps and use it as the second criterion to allocate resources. The number of particles assigned to a group increases with a decrease in the value of Fef f for the group. The third criterion to allocate resources to a group is the mean likelihood of the particles Πmean (gt ) forming the posterior distribution of the group at the previous time instant. The mean likelihood (weights) of the particles determines the effectiveness of the corresponding particle ﬁlter to sample the posterior distribution. A low mean likelihood at the previous time instant necessitates an increase in the number of particles and an increase in the variance of the noise model. This ensures better sampling of the posterior distribution and thus more accurate tracking. If a group gt is a new group formed as a result of a change in the group structure at time t, it will have no corresponding group at the previous time instant. In such cases, gt is assigned a default mean likelihood score.

Πmean (gt ) =

1 Ps

Ps r πt−1 r=1

(10)

The association Assoc(gt ), fraction of effective particles Fef f (gt ) and mean likelihood Πmean (gt ) are linearly combined and used to determine the allocation of particles to each group from the total set of available particles. We also ensure that the number of particles assigned to a group lies within pre-deﬁned minimum and maximum values. These bounding values are based on the size of the group.

5 Experiments
We evaluated our tracking framework on video sequences collected from two surveillance cameras (resolution 320x240 pixels and framerate 15fps). Camera 1 overlooks a pedestrian walkway adjoining a parking lot (Figure 1). Camera 2 overlooks the parking lot (Figure 3). Multiple

people enter and leave the scene. The maximum number of simultaneous people being tracked is 9. The tracking task in camera 2 is quite challenging since the average height of persons in the scene is only 27 pixels, and frequent occlusions are observed due to the large number of people. We compared our tracking framework to two other methods - ﬁrst, multiple independent particle ﬁlters and second, a single joint particle ﬁlter using MCMC as a speed up mechanism [10][18]. In our method, targets within a single interaction group were tracked using a traditional joint particle ﬁlter. However, an MCMC sampling particle ﬁlter can be used here to further speed up tracking. For all three methods, we used the same likelihood function, which is similar to that used in [18]. The maximum number of particles used in the entire scene at every time instant was kept the same for all three methods. Furthermore this upper bound on the number of particles was kept low despite the large number of people in the scene, to ensure a low computational cost, essential for any real time surveillance system. Figure 3 shows tracking results obtained by our particle ﬁltering framework for sample frames from Camera 2. The maximum number of particles was set to 2000 per frame. The top row shows the tracked targets with their IDs, while the bottom row shows the interaction groups formed at the corresponding time instants and the number of particles assigned to each group. Given a small set of particles, our results show the importance of wisely allocating more particles to those groups, whose targets are more prone to be tracked erroneously. The frame in the left column shows a group with a larger number of people (Ids 1,2,3,4) being assigned more particles than smaller groups. In the middle frame, the group of two persons (Ids 1,2) gets assigned a large number of particles per person as compared to the group of 4 persons, due to the high appearance similarity between the targets. The frame in the right column shows an example of a group of two highly occluding targets (Ids 3,4) getting assigned a larger number of particles as compared to a group with three targets (Ids 1,2,5). The other particle ﬁltering frameworks that we compare to, treat all targets equally. Thus, resources are wasted in some parts of the scene, while they are insufﬁcient for other parts. This leads to more tracking errors. Figure 4 shows the histogram of the size of the largest group in every frame for videos from both cameras. For camera 1, the largest group has size 3. For camera 2, the largest group has size 4 although up to 9 persons are simultaneously present in the scene. Each group in the scene is tracked using a particle ﬁlter with a corresponding state space. Thus, the size of the largest group determines the maximum dimensionality of all these state spaces. Note that for a majority of frames in both cameras, groups of at most size 2 are present, which restricts the state spaces to a low dimensionality. Thus, even small sets of particles are able

3 2 5 1 4 6 5 2 1

8 4

7 3 5

2 1

3

4 8

7

9

696 1598 200 200 200 796 1001 594 596

200

Figure 3. Tracking results for Camera 2 shown for three frames. The bottom row shows interaction groups formed by our framework. Groups of the same size are marked with the same color. The number of particles assigned to each group is also displayed. (See text for detailed discussion.)

to densely sample these spaces and provide good results. We measure our tracking accuracy by comparing the predicted bounding boxes to manually marked ground truth locations for all targets. The tracking error for each target is deﬁned as the degree of overlap (Θ) between the predicted bounding box (Bp ) and ground truthed bounding box (Bg ). Θ= Area(Bp ∩ Bg ) Area(Bp ∩ Bg ) + Area(Bp ) Area(Bg )
Camera 1

(11)

Percentage of frames

100 80 60 40 20 0 1 2 3 4 5

Size of the largest group

Percentage of frames

100 80 60 40 20 0 1 2 3 4 5 6 7 8 9

Camera 2

Size of the largest group

Figure 4. Histogram of the size of the largest group in every frame of the video. The size of the largest group determines the maximum dimensionality of the state space.

Large values of Θ indicate a more precise tracking result. When Θ falls below a threshold Θthr for a target, we manually reinitialize the bounding box for that target and resume tracking. The length of a track is then deﬁned as the number of frames between two reinitializations. Figure 5 shows tracking results for both video sequences. The average track length is plotted against Θthr . Clearly, longer track lengths indicate a better system performance. Larger the value of Θthr , lower the error tolerance. This leads to frequent reinitializations giving shorter tracks. Figure 5(a) shows results for Camera 1. The maximum number of particles is set to 500 per frame. The independent particle ﬁlter has many hijackings that take place and needs to be reinitialized often. The joint particle ﬁlter with MCMC sampling also shows a poor performance, as compared to our tracking framework using resource allocation. Figure 5(b) shows results for Camera 2. Here we use a total of 2000 particles per frame. We notice a considerable gain in performance which is very encouraging given that video sequences from Camera 2 are very challenging. The independent particle ﬁlter gives good results when targets are far away from each other, but suffers from hijacking when interactions take place. The joint ﬁlter with MCMC shows poor results because the small set of particles is clearly insufﬁcient for the high dimensional state spaces that need to be sampled, when a large number of people are simultaneously present in the scene. Given the same number of resources, our method clearly outperforms the other two, demonstrating the importance of resource allocation.

250

120

200

Particle Filter with Resource Allocation Joint Particle Filter using MCMC Independent Particle Filters

100

Particle Filter with Resource Allocation Joint Particle Filter using MCMC Independent Particle Filters

[500 particles / frame] Average track length
80

[2000 particles / frame]

Average track length

150

60

100

40

50

20

0 0.75

0.8

0.85

Reinitialization threshold ( Θ )
thr

0.9

0.95

1

1.05

1.1

1.15

0 0.75

0.8

0.85

Reinitialization threshold ( Θ )
thr

0.9

0.95

1

1.05

1.1

1.15

Figure 5. Tracking results for video sequences from Cameras 1 (frames shown in Figure 1) and 2 (frames shown in Figure 3). The system is manually reinitialized when the degree of overlap between the predicted and ground truthed bounding boxes goes below a threshold Θthr . The average length of tracks (measured as the number of frames between two reinitializations) is plotted against Θthr . Our system clearly outperforms the other two tracking methods. (See text for discussion.)

6 Conclusions
We present a particle ﬁltering framework that uses an intelligent resource allocation scheme to track a large number of targets using a small set of particles. The number of particles assigned to each target depends the number of targets it is interacting with, the proximity and appearance models of the interacting targets and the tracking conﬁdence at the previous time instant. We demonstrate the advantages of our method on sequences from two surveillance cameras and compare it to commonly used tracking frameworks.

7 Acknowledgements
This research was funded in part by the U.S. Government VACE program. W. R. Schwartz acknowledges Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES - Brazil, grant BEX1673/04-1). The authors also thank Ryan Farrell and Vlad Morariu for useful discussions.

References
[1] M. S. Arulampalam, S. Maskell, N. Gordon, and T. Clapp. A tutorial on particle ﬁlters for online nonlinear/non-gaussian bayesian tracking. IEEE Trans. Signal Proc., (2), 2002. [2] B. Brandao, J. Wainer, and S. Goldenstein. Subspace hierarchical particle ﬁlter. In SIBGRAPI, pages 194–204, 2006. [3] T. Broida and R. Chellappa. Estimation of object motion parameters from noisy images. IEEE PAMI, 8, 1986. [4] J. Dowdall, I. Pavlidis, and P. Tsiamyrtzis. Coalitional tracking in facial infrared imaging and beyond. In CVPR Workshop, 2006.

[5] A. Gupta, A. Mittal, and L. Davis. Cost: An approach for camera selection and multi-object inference ordering in dynamic scenes. In ICCV, 2007. [6] B. Han, S. Joo, and L. Davis. Probabilistic fusion tracking using mixture kernel-based bayesian ﬁltering. In ICCV, 07. [7] B. Han, Y. Zhu, D. Comaniciu, and L. Davis. Kernel-based bayesian ﬁltering for object tracking. In CVPR, 2005. [8] M. Isard and A. Blake. Condensation - conditional density propagation for visual tracking. IJCV, 1998. [9] M. Isard and J. MacCormick. Bramble: a bayesian multipleblob tracker. ICCV, pages 34–41, 2001. [10] Z. Khan, T. Balch, and F. Dellaert. An mcmc-based particle ﬁlter for tracking multiple interacting targets. In ECCV, 2004. [11] Z. Khan, T. Balch, and F. Dellaert. A rao-blackwellized particle ﬁlter for eigentracking, 2004. [12] J. MacCormick and A. Blake. A probabilistic exclusion principle for tracking multiple targets. In ICCV, 1999. [13] F. Porikli. Integral histogram: A fastway to extract histograms in cartesian spaces. In CVPR, 2005. [14] A. Sankaranarayanan, A. Srivastava, and R. Chellappa. Algorithmic and architectural optimizations for computationally efﬁcient particle ﬁltering. IEEE Trans. Image Proc., 08. [15] C. Yang, R. Duraiswami, and L. Davis. Fast multiple object tracking via a hierarchical particle ﬁlter. In ICCV, 2005. [16] A. Yilmaz, O. Javed, and M. Shah. Object tracking: A survey. ACM Computing Surveys, 38(4), 2006. [17] T. Yu and Y. Wu. Decentralized multiple target tracking using netted collaborative autonomous trackers. In CVPR, 2005. [18] T. Zhao and R. Nevatia. Tracking multiple humans in crowded environment. CVPR, pages 406–413, 2004. [19] S. Zhou, R. Chellappa, and B. Moghaddam. Visual tracking and recognition using appearance-adaptive models in particle ﬁlters. IEEE Trans. Image Proc., 2004.

Tracking Down Under: Following the Satin Bowerbird
Aniruddha Kembhavi†∗, Ryan Farrell∗ , Yuancheng Luo, David Jacobs, Ramani Duraiswami, and Larry S. Davis †Department of Electrical Engineering, Department of Computer Science University of Maryland, College Park, MD 20742 {anikem,yluo1}@umd.edu, {farrell,lsd}@cs.umd.edu, {djacobs,ramani}@umiacs.umd.edu

Abstract
Sociobiologists collect huge volumes of video to study animal behavior (our collaborators work with 30,000 hours of video). The scale of these datasets demands the development of automated video analysis tools. Detecting and tracking animals is a critical ﬁrst step in this process. However, off-the-shelf methods prove incapable of handling videos characterized by poor quality, drastic illumination changes, non-stationary scenery and foreground objects that become motionless for long stretches of time. We improve on existing approaches by taking advantage of speciﬁc aspects of this problem: by using information from the entire video we are able to ﬁnd animals that become motionless for long intervals of time; we make robust decisions based on regional features; for different parts of the image, we tailor the selection of model features, choosing the features most helpful in differentiating the target animal from the background in that part of the image. We evaluate our method, achieving almost 83% tracking accuracy on a more than 200,000 frame dataset of Satin Bowerbird courtship videos.

Figure 1. Satin Bowerbird (Ptilonorhynchus violaceus). A perched male Satin Bowerbird (above right [10]) and two frames taken from overhead courtship videos.

1. Introduction
Sociobiology seeks to understand the social behaviors of a given species by considering the evolutionary advantages these behaviors may have. To observe these social behaviors in their natural setting, biologists conduct a substantial portion of their research in the ﬁeld, recording observations on videotapes. While ﬁeldwork is very demanding, videotape analysis is truly exhausting. The corpus of video footage must be viewed in its entirety, during which time copious notes and qualitative observations are taken. Our collaborators add more than 2000 hours of video annually to a growing total of more than 30,000 hours. They desperately need computational video analysis tools. The approach we have developed addresses the chal∗ These

authors contributed equally.

lenges inherent in detecting and tracking animals in their native outdoor habitats. Characteristics of these ﬁeld observation videos include: poor image quality; drastic illumination changes, some rapid due to varying cloud-cover overhead, others slow and spatial due to shadows cast by the rising sun; targets that are motionless for long stretches of time; and non-stationary background, such as vegetation swaying in the wind and also ground clutter kicked or shifted around by the animals being observed. Conventional computer vision techniques are not yet able to handle all of these challenges simultaneously. Since our goal is to make the Biologist’s video analysis much easier, there are several advantages in our favor. First, the video analysis will take place ofﬂine. This enables us to utilize all the information in the video’s entire space-time volume. Second, we know a priori how many target objects need to be tracked. Third, domain-speciﬁc information about the target’s appearance is available in the form of a coarse target model. Our framework leverages these advantages and overcomes many of these problems. Our main contribution is a staged approach for target detection. We ﬁrst use spatiotemporal volumes to isolate potential target regions. Our al-

gorithm then combines target-speciﬁc information with local scene features to tailor individual models for different parts of the scene. Emphasis is thus given to those features which locally distinguish the target of interest. We demonstrate our framework on an extensive data set of 24 videos comprising a total of more than 200,000 frames where we achieve 82.89% tracking accuracy. These videos contain courtships of the Satin Bowerbird (Ptilonorhynchus violaceus) and were collected by our collaborators, JeanFrancois Savard and Gerald Borgia. ¸ Researchers in Prof. Borgia’s lab study sexual selection (how various traits and behaviors inﬂuence mating success) in various species of the Bowerbird family [2, 13] , generally found in Australia, New Zealand and Southeast Asia. Male Bowerbirds attract mates by constructing a bower, a structure built from sticks and twigs, and decorating the surrounding area. Females visit several bowers before choosing a mating partner and returning to his bower. In part, because both courtship and mating occurs at this known location, Bowerbirds are a particularly good bird in which to study sexual selection. Of particular interest are the adjustments made by the male during courtship in response to the female. Their most recent study [16] evaluates how the male modulates his display (measured as distance from the female) based on the response cues given by a robotic female. An early prototype of our system was very valuable in facilitating the spatial tracking of the courting male, greatly reducing the days of work that would be required for manually tracking so many frames.

2. Related Work
The ﬁrst step towards achieving the biologist’s objectives is to accurately track the animals they are observing. While traditionally done by hand, our goal is to automate the tracking process. A typical method used in computer vision to ﬁnd and track subjects moving within a scene is background subtraction. A sample of representative work includes algorithms based on Gaussian mixture models (Stauffer and Grimson [17]), non-parametric models (Elgammal et al. [4]), and local binary patterns (Heikkil¨ and Pietik¨ inen a a [6]). Typically, background subtraction algorithms are designed for online and sometimes even real-time analysis. These constraints are unnecessary for our purposes, hence affording the ﬂexibility to use all available temporal information in a video, not just information from the recent past. Recent work by Parag et al. [12] takes a similar approach to background modeling, selecting distinctive features on a pixel-by-pixel basis. A crucial advantage of our technique, however, is that we not only pick features that are distinctive for a given location in the scene, we choose the features which most effectively differentiate the target object of interest from that part of the scene. While many effective background subtraction ap-

proaches have been and continue to be proposed, to our knowledge, they all encounter difﬁculties in handling all of the issues of natural outdoor environments such as those in our dataset. The general approach to dealing with background changes such as varying global illumination is to allow the model to evolve, discounting evidence from the more distant past in favor of that just observed. The primary difﬁculty with this method stems from its inability to simultaneously handle foreground objects that become stationary for some period of time (e.g. a sleeping person [18]), instead absorbing them into the background. Efforts have been made to provide tools in support of ﬁeld research. HCI researchers have recently built digital tools that allow biologists to integrate various observations and recordings while in the ﬁeld [20]. In searching for the Ivory-billed Woodpecker, various teams have successfully employed semi-supervised sound analysis software to analyze the large volumes of recordings [5, 7] obtained in the ﬁeld. However, there remains a need for automated tools capable of analyzing video recordings in natural outdoor environments. We are aware of at least two projects that have previously focused on tracking animals. The Biotracking project at Georgia Tech’s Borg Lab has conducted extensive research on multi-target tracking of ants [8] and bees [11] and also tracking larger animals such as rhesus monkey [9]. The SmartVivarium project at UCSD’s Computer Vision Lab has investigated techniques for tracking and behavior analysis of rodents [1]. Their research also includes closely related work on supervised learning of object boundaries [3]. However, in these experiments the animals were observed in captivity, generally under laboratory conditions. While [9] used Stauffer and Grimson’s background modeling technique, we have found this method to work very poorly in the Bowerbird courtship videos.

3. Our Approach
Our approach has three major phases: initial pixel classiﬁcation, pixelwise background model selection and evaluation/ﬁnal classiﬁcation. In the ﬁrst phase, the biologist provides a coarse initial model of the target (a male Bowerbird in our case) that he/she wishes to track throughout the video. This model is used to segment each frame of the video, extracting possible target pixels (in reality some target, some background), ideally leaving behind a set of only background pixels 1 . Here, we use information from all previous and all future frames of the video to take decisions (as opposed to just a few frames from the past). This helps us overcome the problem of the Bowerbird often being stationary for hundreds, even thousands of frames at a time.
1 We deﬁne background pixels to be all those pixels that are not part of the target indicated by the biologist.

A key characteristic of unconstrained outdoor videos is the variation of the background scene, both from video to video as well as from one part of the image to another. Our second phase accounts for this. Here, we use the sets of background and target pixels and Principal Component Analysis (PCA) on a bag of features, to choose different features at different locations in the image, which can be used to build robust models. Our bag of features includes some that incorporate neighborhood information. In the third phase, we use non-parametric Kernel Density Estimation (KDE) to build a background model for each individual image location (pixel). We then evaluate this pixel’s value over all frames in the video, determining the probability in each frame that the pixel belongs to this model. We explain these three phases in greater detail in the following subsections.

els around it are less likely to simultaneously ﬁt the model as well. Third, our use of regional information allows us to “see through” occluding surfaces such as branches and foliage when the target is passing beneath them.

Module 1: Initial Pixel Classification
al Vid eo

F

m ra

es

of

O

in rig

Or igi

na lI m

ag

e

3.1. Initial Pixel Classiﬁcation
Many of the videos in our dataset are affected by drastic changes in global illumination. These are caused by varying levels of cloud cover and by sunlight ﬁltering through the canopy and foliage above. The automatic gain control setting on the camera also produces sudden global changes in the color and brightness of the video. To deal with such global illumination changes, we transform every image from RGB color space into a one dimensional rankordered space, equivalent to performing histogram equalization on the grayscale image. The rank feature space assumes that the feature distribution changes very little, instead just shifting due to a change in the overall illumination. It disregards the absolute brightness of a pixel in the scene, rather considering only its value relative to all the pixels in the image. It is invariant to multiplicative and additive global changes and thus is largely unaffected by these effects we have observed. In order to tune our system to track the target, we require an initialization by the biologist. Before a video is processed, the biologist analyzes a small number of frames chosen randomly, and marks out the region enclosing the Bowerbird at every frame where it is present. These pixels are used to build a smoothed histogram to serve as a coarse initial model of the target. This model is used to classify every pixel in the video into one of two sets - “potential” target pixels and “high conﬁdence” background pixels. At each image location, the feature that is used for this initial pixel classiﬁcation is a neighborhood histogram of rank intensity. While most traditional background subtraction approaches have relied on the information contained at a single pixel to build background and target models, we rely more on neighborhood information for the following reasons. First, it reduces the chance of noisy pixels being classiﬁed as target pixels. Second, while some background pixels might closely ﬁt the target model, neighboring pix-

I nk Ra

ma

P ge

h atc

es

for

p

Ra Ra

nk

Im

ag e

nk

Im for ag pix e Pa el tch p

am gr to l p is ixe H p h tc for Pa

s

Fo

re g Moroun de d l

Dot Product

Dot Product

0 50

00 20 ber m 0 50 e Nu 1 m 0 00 Fra 1

00 25

0 30

0

0 cla s ti ica sif

on

of

p

In

itia

e lp

ra r-f

me

Figure 2. Module 1: Initial Pixel Classiﬁcation.

Consider a tube of pixels pij = {pt }, where (i, j) deij note the spatial location and t ∈ {1, 2, .., T } denotes the frame number in the video sequence. We calculate a histogram of the neighboring patch at every time step to obtain a sequence of patch histograms as shown in Figure 2. Every histogram in this set is projected onto the target model to obtain a 1-D time series as shown in Figure 2. A high response at certain times indicates probable presence of the target at those times in the neighborhood of pixel (i, j). This process

Module 2: Pixelwise Background Model Selection
Po t en

Module 3: Evaluation and Final Classification
True Background

ra r-f Pe

lab me

of ng eli

p

tia Fo lly c re on gr ou tain nd s

H of igh C Pu on re fid Ba en ck ce gr ou nd

Misclassified Background True Foreground

Project Feature Vectors into the PCA subspace

Obtain subsets by sampling

Build Non-parametric Model (KDE) and Evaluate Probabilities
of p

Perform Principle Component Analysis (PCA)

E (o ige bt nv ain ec ed tor by s PC

ra r-f pe al Fin

me

ati ific ss cla

on

Final Classification: Identify the frames where foreground is present at pixel p

A)

Figure 3. Module 2: Pixelwise Background Model Selection and Module 3: Evaluation and Final Classiﬁcation.

is repeated for every pixel location (i, j). In summary, we identify pixels whose neighborhoods, at times, change to more closely resemble the target model. Using all patches, both the past and the future frames, has its advantages. In the videos in our dataset, the Bowerbird jumps suddenly from one location to another, and then often waits at a single location for a lengthy period of time, sometimes even thousands of frames. Using a small quantile of the time series to model the response of background patches, we are able to easily identify frames when the bird might have visited the immediate neighborhood. We take great care not to allow target to be mixed with the background. This hypersensitivity in initial classiﬁcation reduces the number of false negative target classiﬁcations at the cost of marginally increased false positive rates. At each pixel this gives us two sets, Fij and Bij , consisting of the frame numbers that are respectively classiﬁed as potential target and high conﬁdence background pixels. In essence, we obtain an over-background-subtracted sequence of images. We can now use the reliable set Bij to build more complex and robust background models.

ground models (R,G,B and gray values, gradients, edges and even texture measures). However in outdoor videos, such as the ones in our dataset, the background varies greatly in different parts of the scene as well as across different videos. The additional knowledge we have about the appearance of the target object should also play an important role in determining which features would be most effective at different places in the image. For example, sometimes the bird walks over grass-ﬁlled regions, where color might be an important cue. At other times, it walks over bright sunlit areas, where a histogram of neighborhood intensities might differentiate it. For highly textured targets, a bank of oriented ﬁlters might be appropriate. We utilize information about pixels from both sets, potential target and high conﬁdence background, and choose the most appropriate features for every pixel location from a “bag of features”. Consider pixel pt . At every time step t, we concatenate ij t multiple features to form a joint feature vector fij . These could include any pixel-based or neighborhood-based features. We next determine which elements of the feature vectors are most important for distinguishing target and background pixels at location (i, j) for times t = {1, 2, .., T }. The set of potential target pixels has a large number of background pixels in it, because of the conservative thresholds we choose for the initial pixel classiﬁcation. This prevents

3.2. Pixelwise Background Model Selection
Traditional background subtraction techniques rely on a ﬁxed set of features to build their background and fore-

us from using a standard hard classiﬁer to label the pixels as target and background. Instead, we use PCA to project our feature vectors onto a subspace that maximizes the variance, and KDE to classify them. This probabilistic framework allows us to remove many of the falsely classiﬁed pixels from the potential target set. We only use a small sample of feature vectors from the target set Fij and from the background set Bij to obtain a reduced subspace, as shown in Figure 3. Projecting the entire feature set fij onto this subspace gives us the set rij , in the reduced space. The reduced dimensionality of rij helps to drastically reduce the time required to build background models.

3.3. Evaluation and Final Classiﬁcation
For every pixel we build a background model using Kernel Density Estimation on our reduced feature set and evaluate probabilities at all time frames that were initially classiﬁed as potential target Fij . Suitably thresholding these probabilities allows us to break down the set Fij into a set of target pixels and pixels that were misclassiﬁed as target by the ﬁrst module of our system. For t ∈ Bij (background), s ∈ Fij (potential target) and kernel K, we obtain:
s P (rij ) =

1 N σ1 ..σd

d

K
t∈Bij y=1

s t rij,y − rij,y σy

(1)

This gives us a target silhouette at every frame of the video sequence, from which we are able to calculate the centroid of the detected region at every time step. We compare these centroid locations to ground truth provided to us by the biologists, and present our results in the following section.

also over a large temporal interval (the entire video). Computing statistics for each image pixel over this large temporal window requires a tremendous amount of data storage. The amount of memory needed to store a single byte per pixel over 10,000 VGA sized frames is 2.86GB. We compute feature vectors per pixel that would require about 100 or more bytes of memory per pixel (25 or more ﬂoatingpoint features). If this entire structure were to be in memory at one time, it would require 100s of GB of memory, rendering this task impossible for even a modern PC. We are further-constrained by the memory limits of a 32-bit version of MATLAB (only about 1.2GB are available for variables). These considerations led us to implement our processing using data-decomposition as is frequently done in high performance scientiﬁc computing (though we process a given video serially on a single machine where a distributed system would run in parallel). We utilize two kinds of datastructures, tubes and chunks. Tubes refer to spatial subdivisions of the video (entire space-time volume), such that all frames for a particular subregion of the image ﬁt simultaneously in memory. Chunks are temporal subdivisions, a contiguous set of frames in time that simultaneously ﬁt into memory. These tubes and chunks must be created for not only the original image frames of the video but also for the large data structures that we accumulate during processing. At different stages, our algorithm requires reading in all the data, on a tube-by-tube or a chunk-by-chunk basis.

5. Experimental Evaluation
We evaluate our framework on a dataset of 24 videos comprising a total of over 200,000 frames captured at 29.97fps and a resolution of 720x480 pixels. The length of the bowerbird in the frames is roughly 90 pixels. While manually specifying the ground truth centroid for each of 200,000 frames is infeasible, we are fortunate to have what we consider a close second. In their study [16], our collaborators used an application implementing a very early prototype of our bowerbird tracking software. This version included a provision to manually correct erroneous tracking results. The biologists went through and reﬁned the automatic tracking results for every frame in the dataset such that all centroids were then within the acceptable tolerance of 4.5cm in the real world (about 15 pixels in the image). We use these manually corrected results as our “ground truth” to quantitatively assess of our approach. Given this, we seek to evaluate our approach using the following metrics: overall accuracy, per-video percentage within the biologist-speciﬁed tolerance, and false-positive and false-negative rates. In Figure 4(a), we present a cumulative distribution of overall accuracy. All videos are superimposed and the required tolerance is shown by the red dotted line. The overall cumulative distribution is shown by the solid line. Figure 4(b) shows the per-video percent-

4. Computational Considerations
Our implementation of the framework described in Section 3 incorporates highly optimized algorithms to facilitate the processing of these large videos. We utilize Integral Histograms [14] both to generate the patch histograms used in pixel classiﬁcation and to generate features for background model selection. Further, to optimize the evaluation stage, we build KDEs and determine probabilities using the Improved Fast Gauss Transform (IFGT) [15, 19]. The framework is implemented in MATLAB, with computation- and memory-intensive algorithms such as Integral Histograms and IFGT implemented in C++ and compiled as mex routines. In addition to these algorithmic optimizations, we also employed many workstations2 (a subset of the vnode cluster funded through NSF Infrastructure Grant CNS 043313) to process multiple videos in parallel. A key strength of our background modeling approach is the use of a large spatio-temporal window. We consider image statistics, both in a large region around a given pixel and
2 Workstations

have dual 3.0Ghz Intel Xeon processors, 8GB RAM

100 90

Percentage of centroids

80 70 60 50 40 30 20 10 0 0 5 10 15

Error (pixels)

20

25

30

35

40

45

(a) Overall Accuracy

(b) Percentage within Tolerance

Figure 4. Evaluation (a) Cumulative distribution of accuracy of every video is shown by faint lines. Overall accuracy is shown by the solid line. Red dotted line denotes the accuracy (in terms of centroid detection error) required by the biologists. (b) Per-video percentage of centroids within the biologists speciﬁed tolerance.

age of centroids within the speciﬁed tolerance. Overall, we are able to track the target within the biologists error tolerance in 82.89% of the frames in our dataset. For most of our videos this number goes beyond 90%. Having to hand label thousands of frames per video, biologists often spend days just tracking the object of interest. An accuracy of over 90% represents a very signiﬁcant reduction in the time required for this process. We obtain overall false positive and false negative detection rates of 4.8% and 3.44% respectively. Our false positive detections are primarily caused by moving shadows cast by the overlying trees, and our false negative detections are primarily caused by severe occlusions by large branches and shrubs in the scene. It is often easier to manually correct false positives as compared to false negatives. The biologist can mark out a sequence of frames when the target is not present in the scene and all false positives within that range can be ignored. Figure 4(b) shows poor results for three of the videos in the dataset. These are caused by severe occlusions by large shrubs in the scene, making it very difﬁcult to locate the target accurately. Figure 5(a) shows a few frames from one of the videos in the database, sampled approximately every 300 frames. The stark illumination changes from one part of the video to another can be clearly seen. Figure 5(b) and Figure 5(c) show the results of the two modules in our staged approach to target detection. Some of the videos also had a very poor contrast between the target and background pixels, due to the dark shadows cast by the overlying trees, and the dark color of the male bowerbird. Figure 6 shows an example frame and detection results from one such video.

regions and then combine target-speciﬁc information with local scene features to tailor individual models for different parts of the scene. Our collaborators used an earlier prototype of our implementation for their study, and saved a considerable amount of time that they would have spent to manually track the target in every frame. We obtain accurate tracking within the biologists’ error tolerance in over 90% of frames for many of the videos in our dataset.

Acknowledgements
We wish to express our appreciation to our collaborators, Jean-Francois Savard and Gerald Borgia. This research was ¸ funded in part by the U.S. Government VACE program.

References
[1] K. Branson and S. Belongie. Tracking multiple mouse contours (without too many samples). In CVPR, 2005. [2] S. W. Coleman, G. L. Patricelli, and G. Borgia. Variable female preferences drive complex male displays. Nature, 428(6984), 2004. [3] P. Dollar, Z. Tu, and S. Belongie. Supervised learning of edges and object boundaries. In CVPR, 2006. [4] A. M. Elgammal, D. Harwood, and L. S. Davis. Non-parametric model for background subtraction. In ECCV (2), 2000. [5] J. W. Fitzpatrick et al. Ivory-billed Woodpecker (Campephilus principalis) Persists in Continental North America. Science, 308(5727):1460–1462, 2005. [6] M. Heikkil¨ and M. Pietik¨ inen. A texture-based method for modela a ing the background and detecting moving objects. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4):657–662, 2006. [7] G. E. Hill, D. J. Mennill, B. W. Rolek, T. L. Hicks, and K. A. Swiston. Evidence suggesting that ivory-billed woodpeckers (Campephilus principalis) exist in ﬂorida. Avian Conservation and Ecology, ’06. [8] Z. Khan, T. R. Balch, and F. Dellaert. An mcmc-based particle ﬁlter for tracking multiple interacting targets. In ECCV, 2004. [9] Z. Khan, R. A. Herman, K. Wallen, and T. Balch. An outdoor 3-d visual tracking system for the study of spatial navigation and memory in rhesus monkeys. Behavior Research Methods, 37, August 2005. [10] C. Moores. www.charliesbirdblog.com, used with permission. [11] S. M. Oh, J. M. Rehg, T. Balch, and F. Dellaert. Learning and inference in parametric switching linear dynamical systems. In ICCV’05, Washington, DC, USA.

6. Conclusion
Sociobiologists collect thousands of hours of video to study animal behavior. Detecting and tracking animals is a critical ﬁrst step in this process. We improve on existing techniques with a two staged approach to target detection. We use spatiotemporal volumes to isolate potential target

(a) Frames from one of the videos in the database showing the male bowerbird to be tracked. Notice the stark illumination and color changes in the sequence.

(b) Initial pixel classiﬁcation by Module 1 for the above frames. The shaded pixels are classiﬁed as potential target pixels. They include a large number of background pixels as well due to the conservative thresholds set in Module 1.

(c) Final results for the above frames. The detected centroid of the target is marked with a green dot, and the ground truth is shown in red.

Figure 5. Target detection for a sequence with stark illumination changes.

Figure 6. Target detection for a sequence with poor contrast between target and background.

[12] T. Parag, A. M. Elgammal, and A. Mittal. A framework for feature selection for background subtraction. In CVPR, 2006. [13] G. L. Patricelli, J. A. C. Uy, G. Walsh, and G. Borgia. Sexual selection: Male displays adjusted to female’s response. Nature, 415(6869):279–280, 2002. [14] F. Porikli. Integral histogram: A fast way to extract histograms in cartesian spaces. CVPR, 1:829–836, 2005. [15] V. C. Raykar and R. Duraiswami. Fast optimal bandwidth selection for kernel density estimation. In SDM, 2006. [16] J.-F. Savard, J. Keagy, and G. Borgia. Spatial dynamics and modulation of courtship in satin bowerbirds, Ptilonorhynchus violaceus.

44th annual meeting of the Animal Behavior Society, 2007. [17] C. Stauffer and W. E. L. Grimson. Adaptive background mixture models for real-time tracking. In CVPR, 1999. [18] K. Toyama, J. Krumm, B. Brumitt, and B. Meyers. Wallﬂower: Principles and practice of background maintenance. In ICCV, 1999. [19] C. Yang, R. Duraiswami, and L. S. Davis. Efﬁcient kernel machines using the improved fast gauss transform. In NIPS, 2004. [20] R. Yeh, C. Liao, S. Klemmer, F. Guimbreti` re, B. Lee, B. Kakaradov, e J. Stamberger, and A. Paepcke. Butterﬂynet: a mobile capture and access system for ﬁeld biology research. In Proceedings of the SIGCHI conference on Human Factors in computing systems, 2006.

